{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bbfd73",
   "metadata": {},
   "source": [
    "### === USER CONFIGURATION ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === USER CONFIGURATION ===\n",
    "# Path to downloaded product\n",
    "PATH_L1B_DATA = \"PATH/TO/L1B/DATA\"\n",
    "\n",
    "# Output folder chosen by the user\n",
    "OUTPUT_FOLDER = \"PATH/TO/OUTPUT/FOLDER\" \n",
    "\n",
    "# Path to geoid file : OPTIONNAL\n",
    "GEOID_PATH = \"\"\n",
    "\n",
    "\n",
    "# === SEN2VM OPTIONS ===\n",
    "\n",
    "GRID_MODE = \"direct\"  # direct or inverse\n",
    "MOVE_GIPP = True   # True = move extracted GIPP dirs directly under S2A/S2B/S2C\n",
    "                   # False = keep them inside GIP_* folders\n",
    "IERS_TYPE = \"A\"\n",
    "UTM_EPSG = 23037  # UTM zone EPSG code for the ROI\n",
    "LOCATION = {\n",
    "    \"ul_x\": -235763.5,\n",
    "    \"ul_y\": 3344817.1,\n",
    "    \"lr_x\": -234763.5,\n",
    "    \"lr_y\": 3345817.1\n",
    "}\n",
    "STEPS = {\n",
    "    \"10m_bands\": 10,\n",
    "    \"20m_bands\": 20,\n",
    "    \"60m_bands\": 60\n",
    "}\n",
    "\n",
    "\n",
    "# === GDAL ORTHO OPTIONS ===\n",
    "ORTHO_SETTINGS = {\n",
    "    \"enabled\": True,                 # True = do orthorectification\n",
    "    \"keep_bands\": [\"B01\" , \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\"],\n",
    "    \"resolution_mode\": \"global\",     # \"global\" or \"per_band\"\n",
    "    \"global_resolution\": 10,         # used if resolution_mode = \"global\"\n",
    "\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GIPPs ===\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "root_dir = os.getcwd()               \n",
    "src_dir = root_dir                   \n",
    "\n",
    "# =====================================================================\n",
    "# 1) CLONE sen2vm-gipp-database\n",
    "# =====================================================================\n",
    "\n",
    "gipp_repo_name = \"sen2vm-gipp-database\"\n",
    "gipp_repo_path = os.path.join(src_dir, gipp_repo_name)\n",
    "\n",
    "# Delete repository if exists\n",
    "if os.path.exists(gipp_repo_path):\n",
    "    print(f\"Removing existing repository: {gipp_repo_path}\")\n",
    "    shutil.rmtree(gipp_repo_path)\n",
    "\n",
    "# Clone repository fresh\n",
    "print(\"Cloning sen2vm-gipp-database...\")\n",
    "!git clone https://github.com/sen2vm/sen2vm-gipp-database.git {gipp_repo_path}\n",
    "print(\"Clone complete.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2) CLONE sen2vm-core \n",
    "# =====================================================================\n",
    "\n",
    "print(\"cloning sen2vm-core...\")\n",
    "\n",
    "core_repo_name = \"sen2vm-core\"\n",
    "core_repo_path = os.path.join(src_dir, core_repo_name)\n",
    "\n",
    "# Delete existing repo if present\n",
    "if os.path.exists(core_repo_path):\n",
    "    print(f\"Removing existing repository: {core_repo_path}\")\n",
    "    shutil.rmtree(core_repo_path)\n",
    "\n",
    "# Clone core repo\n",
    "!git clone https://github.com/sen2vm/sen2vm-core.git {core_repo_path}\n",
    "print(\"sen2vm-core clone complete.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# 3) EXTRACT PRODUCT DATE\n",
    "# =====================================================================\n",
    "\n",
    "match = re.search(r\"_V(\\d{8}T\\d{6})_\", PATH_L1B_DATA)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Could not extract product date from L1B filename.\")\n",
    "\n",
    "product_date_str = match.group(1)\n",
    "product_date = datetime.strptime(product_date_str, \"%Y%m%dT%H%M%S\")\n",
    "\n",
    "print(\"Product date extracted:\", product_date_str, \"\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 4) PARSE ALL GIPP FILES AND GROUP THEM\n",
    "# =====================================================================\n",
    "\n",
    "def parse_gipp_filename(filename):\n",
    "    pattern = (\n",
    "        r\"^(S2[A-C])_OPER_GIP_(\\w+)_MPC__\"\n",
    "        r\"(\\d{8}T\\d{6})_V(\\d{8}T\\d{6})_(\\d{8}T\\d{6})_(B[\\dA-Z]+)\\.TGZ$\"\n",
    "    )\n",
    "    m = re.match(pattern, filename)\n",
    "    if not m:\n",
    "        return None\n",
    "\n",
    "    return {\n",
    "        \"mission\": m.group(1),\n",
    "        \"gip_type\": m.group(2),\n",
    "        \"gen_date\": datetime.strptime(m.group(3), \"%Y%m%dT%H%M%S\"),\n",
    "        \"valid_from\": datetime.strptime(m.group(4), \"%Y%m%dT%H%M%S\"),\n",
    "        \"valid_to\": datetime.strptime(m.group(5), \"%Y%m%dT%H%M%S\"),\n",
    "        \"band\": m.group(6),\n",
    "    }\n",
    "\n",
    "grouped = {}\n",
    "\n",
    "for mission in [\"S2A\", \"S2B\", \"S2C\"]:\n",
    "    mission_dir = os.path.join(gipp_repo_path, mission)\n",
    "    for gip_dir in os.listdir(mission_dir):\n",
    "        gip_path = os.path.join(mission_dir, gip_dir)\n",
    "        if not os.path.isdir(gip_path):\n",
    "            continue\n",
    "\n",
    "        for f in os.listdir(gip_path):\n",
    "            if not f.endswith(\".TGZ\"):\n",
    "                continue\n",
    "\n",
    "            info = parse_gipp_filename(f)\n",
    "            if not info:\n",
    "                print(\"Skipping unrecognized GIPP:\", f)\n",
    "                continue\n",
    "\n",
    "            key = (info[\"mission\"], info[\"gip_type\"], info[\"band\"])\n",
    "            grouped.setdefault(key, [])\n",
    "            grouped[key].append({\n",
    "                \"filename\": f,\n",
    "                \"path\": os.path.join(gip_path, f),\n",
    "                \"valid_from\": info[\"valid_from\"]\n",
    "            })\n",
    "\n",
    "print(\"GIPP files grouped.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 5) SELECT CORRECT GIPP FOR EACH GROUP\n",
    "# =====================================================================\n",
    "\n",
    "selected = []\n",
    "\n",
    "for key, items in grouped.items():\n",
    "    valid_items = [it for it in items if it[\"valid_from\"] <= product_date]\n",
    "    if not valid_items:\n",
    "        continue\n",
    "\n",
    "    best = max(valid_items, key=lambda x: x[\"valid_from\"])\n",
    "    selected.append(best)\n",
    "\n",
    "print(f\"Selected {len(selected)} GIPP files.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 6) REMOVE NON-SELECTED TGZ\n",
    "# =====================================================================\n",
    "\n",
    "selected_files = set(item[\"path\"] for item in selected)\n",
    "\n",
    "for mission in [\"S2A\", \"S2B\", \"S2C\"]:\n",
    "    mission_dir = os.path.join(gipp_repo_path, mission)\n",
    "\n",
    "    for gip_dir in os.listdir(mission_dir):\n",
    "        gip_path = os.path.join(mission_dir, gip_dir)\n",
    "        for f in os.listdir(gip_path):\n",
    "            fpath = os.path.join(gip_path, f)\n",
    "\n",
    "            if f.endswith(\".TGZ\") and fpath not in selected_files:\n",
    "                print(\"Removing outdated GIPP:\", fpath)\n",
    "                os.remove(fpath)\n",
    "\n",
    "print(\"Old GIPP removed.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 7) UNTAR SELECTED GIPP\n",
    "# =====================================================================\n",
    "\n",
    "for item in selected:\n",
    "    tgz_path = item[\"path\"]\n",
    "    dst_dir = os.path.dirname(tgz_path)\n",
    "\n",
    "    print(\"Extracting:\", tgz_path)\n",
    "    with tarfile.open(tgz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(dst_dir)\n",
    "\n",
    "    os.remove(tgz_path)\n",
    "\n",
    "print(\"\\nTGZ extraction complete.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 8) OPTIONAL MOVE OF GIPP FILES\n",
    "# =====================================================================\n",
    "\n",
    "if MOVE_GIPP:\n",
    "    print(\"Moving extracted GIPP into mission folders...\\n\")\n",
    "\n",
    "    for mission in [\"S2A\", \"S2B\", \"S2C\"]:\n",
    "        mission_dir = os.path.join(gipp_repo_path, mission)\n",
    "\n",
    "        for gip_dir in os.listdir(mission_dir):\n",
    "            gip_path = os.path.join(mission_dir, gip_dir)\n",
    "\n",
    "            if not os.path.isdir(gip_path):\n",
    "                continue\n",
    "\n",
    "            for f in os.listdir(gip_path):\n",
    "                fpath = os.path.join(gip_path, f)\n",
    "                if os.path.isfile(fpath):\n",
    "                    shutil.move(fpath, mission_dir)\n",
    "\n",
    "            if not os.listdir(gip_path):\n",
    "                os.rmdir(gip_path)\n",
    "\n",
    "    print(\"MOVE_GIPP complete.\\n\")\n",
    "\n",
    "print(\"All GIPP processing finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD IERS ===\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd()) \n",
    "DATA_DIR = os.path.join(root_dir, \"DATA\")\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise RuntimeError(f\"DATA directory not found: {DATA_DIR}\")\n",
    "\n",
    "print(\"Bulletin output directory:\", DATA_DIR)\n",
    "\n",
    "# =====================================================================\n",
    "# 0. REMOVE EXISTING IERS BULLETINS\n",
    "# =====================================================================\n",
    "\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.startswith(\"bulletina-\") and f.endswith(\".txt\"):\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        print(\"Removed old bulletin A:\", f)\n",
    "\n",
    "    if f.startswith(\"bulletinb-\") and f.endswith(\".txt\"):\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        print(\"Removed old bulletin B:\", f)\n",
    "\n",
    "print(\"Cleanup of old bulletins complete.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2. EXTRACT PRODUCT DATE\n",
    "# =====================================================================\n",
    "\n",
    "match = re.search(r\"_V(\\d{8}T\\d{6})_\", PATH_L1B_DATA)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Could not extract product date from L1B filename.\")\n",
    "\n",
    "product_date_str = match.group(1)\n",
    "year = int(product_date_str[:4])\n",
    "month = int(product_date_str[4:6])\n",
    "day = int(product_date_str[6:8])\n",
    "\n",
    "product_date = datetime(year, month, day)\n",
    "print(\"Product date:\", product_date.date(), \"\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# Roman conversion \n",
    "# =====================================================================\n",
    "\n",
    "def int_to_roman(n):\n",
    "    vals = [\n",
    "        (1000, 'm'), (900, 'cm'), (500, 'd'), (400, 'cd'),\n",
    "        (100, 'c'), (90, 'xc'), (50, 'l'), (40, 'xl'),\n",
    "        (10, 'x'), (9, 'ix'), (5, 'v'), (4, 'iv'), (1, 'i')\n",
    "    ]\n",
    "    res = \"\"\n",
    "    for v, s in vals:\n",
    "        while n >= v:\n",
    "            res += s\n",
    "            n -= v\n",
    "    return res\n",
    "\n",
    "# =====================================================================\n",
    "# Bulletin B\n",
    "# =====================================================================\n",
    "\n",
    "if IERS_TYPE.upper() == \"B\":\n",
    "\n",
    "    bulletin_number = 143 + (year - 2000)*12 + (month - 1)\n",
    "    url = f\"https://datacenter.iers.org/data/207/bulletinb-{bulletin_number}.txt\"\n",
    "    dest_file = os.path.join(DATA_DIR, f\"bulletinb-{bulletin_number}.txt\")\n",
    "\n",
    "    print(\"Using Bulletin B:\", os.path.basename(dest_file))\n",
    "\n",
    "    response = requests.get(url)\n",
    "    if response.status_code != 200:\n",
    "        raise RuntimeError(f\"Bulletin B {bulletin_number} not available (HTTP {response.status_code}).\")\n",
    "\n",
    "    with open(dest_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(\"Downloaded:\", dest_file)\n",
    "\n",
    "# =====================================================================\n",
    "# Bulletin A\n",
    "# =====================================================================\n",
    "\n",
    "else:\n",
    "\n",
    "    roman_year = int_to_roman(year - 1987)\n",
    "    doy = product_date.timetuple().tm_yday\n",
    "    index = (doy - 1) // 7 + 1\n",
    "\n",
    "    print(f\"Bulletin A Roman year: {roman_year}\")\n",
    "    print(f\"Initial weekly index: {index}\\n\")\n",
    "\n",
    "    found = False\n",
    "\n",
    "    while index > 0:\n",
    "        index_str = f\"{index:03d}\"\n",
    "        url = f\"https://datacenter.iers.org/data/6/bulletina-{roman_year}-{index_str}.txt\"\n",
    "        print(\"Trying bulletin:\", url)\n",
    "\n",
    "        response = requests.get(url)\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            print(\"Bulletin found:\", index_str)\n",
    "            dest_file = os.path.join(DATA_DIR, f\"bulletina-{roman_year}-{index_str}.txt\")\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "        index -= 1\n",
    "\n",
    "    if not found:\n",
    "        raise RuntimeError(\"No Bulletin A available for current or previous weeks.\")\n",
    "\n",
    "    with open(dest_file, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    print(\"Downloaded:\", dest_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da2b8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276592b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE CONFIG.JSON  ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "USERCONF_DIR = os.path.join(root_dir, \"UserConf\")\n",
    "os.makedirs(USERCONF_DIR, exist_ok=True)\n",
    "\n",
    "print(\"UserConf directory:\", USERCONF_DIR)\n",
    "\n",
    "# =====================================================\n",
    "# 1. Extract mission from PATH_L1B_DATA\n",
    "# =====================================================\n",
    "match = re.match(r\".*/(S2[A-C])_.*\\.SAFE$\", PATH_L1B_DATA)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Cannot extract mission (S2A/S2B/S2C) from PATH_L1B_DATA.\")\n",
    "\n",
    "mission = match.group(1)\n",
    "\n",
    "# =====================================================\n",
    "# 2. Docker paths inside /workspace\n",
    "# =====================================================\n",
    "docker_l1b  = \"/workspace/DATA/\" + os.path.basename(PATH_L1B_DATA)\n",
    "docker_dem  = \"/workspace/DATA/DEM\"\n",
    "docker_gipp = f\"/workspace/src/sen2vm-gipp-database/{mission}\"\n",
    "\n",
    "# =====================================================\n",
    "# 3. Resolve geoid path\n",
    "# =====================================================\n",
    "if GEOID_PATH.strip() == \"\" or GEOID_PATH is None:\n",
    "    docker_geoid = (\n",
    "        \"/workspace/src/sen2vm-core/src/test/resources/DEM_GEOID/\"\n",
    "        \"S2__OPER_DEM_GEOIDF_MPC__20200112T130120_S20190507T000000.gtx\"\n",
    "    )\n",
    "else:\n",
    "    # Convert host absolute path → docker relative under /workspace\n",
    "    docker_geoid = \"/workspace/\" + GEOID_PATH.replace(root_dir + \"/\", \"\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. Locate IERS bulletin on host\n",
    "# =====================================================\n",
    "\n",
    "DATA_DIR = os.path.join(root_dir, \"DATA\")\n",
    "iers_host = None\n",
    "\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.startswith(\"bulletin\"):\n",
    "        iers_host = os.path.join(DATA_DIR, f)\n",
    "        break\n",
    "\n",
    "if iers_host is None:\n",
    "    raise RuntimeError(\"IERS bulletin not found inside DATA directory.\")\n",
    "\n",
    "docker_iers = \"/workspace/DATA/\" + os.path.basename(iers_host)\n",
    "\n",
    "# =====================================================\n",
    "# 5. Build config dictionary\n",
    "# =====================================================\n",
    "\n",
    "config = {\n",
    "    \"l1b_product\": docker_l1b,\n",
    "    \"gipp_folder\": docker_gipp,\n",
    "    \"gipp_check\": True,\n",
    "    \"grids_overwriting\": True,\n",
    "    \"dem\": docker_dem,\n",
    "    \"geoid\": docker_geoid,\n",
    "    \"iers\": docker_iers,\n",
    "    \"operation\": GRID_MODE,\n",
    "    \"deactivate_available_refining\": False,\n",
    "    \"steps\": {\n",
    "        \"10m_bands\": STEPS[\"10m_bands\"],\n",
    "        \"20m_bands\": STEPS[\"20m_bands\"],\n",
    "        \"60m_bands\": STEPS[\"60m_bands\"]\n",
    "    },\n",
    "    \"export_alt\": True\n",
    "}\n",
    "\n",
    "# Add inverse block only if needed\n",
    "if GRID_MODE == \"inverse\":\n",
    "    config[\"inverse_location_additional_info\"] = {\n",
    "        \"ul_x\": float(LOCATION[\"ul_x\"]),\n",
    "        \"ul_y\": float(LOCATION[\"ul_y\"]),\n",
    "        \"lr_x\": float(LOCATION[\"lr_x\"]),\n",
    "        \"lr_y\": float(LOCATION[\"lr_y\"]),\n",
    "        \"referential\": UTM_EPSG,\n",
    "        \"output_folder\": \"/workspace/DATA/Output\"\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# Save config.json\n",
    "# =====================================================\n",
    "\n",
    "config_path = os.path.join(USERCONF_DIR, \"config.json\")\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"Configuration file generated:\")\n",
    "print(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE PARAMS.JSON ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "USERCONF_DIR = os.path.join(root_dir, \"UserConf\")\n",
    "os.makedirs(USERCONF_DIR, exist_ok=True)\n",
    "\n",
    "print(\"UserConf directory:\", USERCONF_DIR)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Locate GRANULE folders\n",
    "# -----------------------------------------------------\n",
    "GR_TARGET_DIR = os.path.join(PATH_L1B_DATA, \"GRANULE\")\n",
    "\n",
    "if not os.path.exists(GR_TARGET_DIR):\n",
    "    raise RuntimeError(\"GRANULE directory not found inside L1B SAFE.\")\n",
    "\n",
    "granule_folders = [\n",
    "    os.path.join(GR_TARGET_DIR, d)\n",
    "    for d in os.listdir(GR_TARGET_DIR)\n",
    "    if os.path.isdir(os.path.join(GR_TARGET_DIR, d))\n",
    "]\n",
    "\n",
    "print(\"Found\", len(granule_folders), \"granule folders.\")\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Extract detectors and bands from JP2\n",
    "# -----------------------------------------------------\n",
    "detectors = set()\n",
    "bands = set()\n",
    "\n",
    "pattern = r\"_D(\\d+)_B(\\d{1,2}[A]?)\\.jp2$\"\n",
    "\n",
    "for granule in granule_folders:\n",
    "    img_data_dir = os.path.join(granule, \"IMG_DATA\")\n",
    "\n",
    "    if not os.path.isdir(img_data_dir):\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(img_data_dir):\n",
    "        match = re.search(pattern, fname)\n",
    "        if match:\n",
    "            detectors.add(match.group(1))\n",
    "            bands.add(f\"B{match.group(2)}\")\n",
    "\n",
    "detectors = sorted(detectors)\n",
    "bands = sorted(bands)\n",
    "\n",
    "print(\"Detected detectors:\", detectors)\n",
    "print(\"Detected bands:\", bands)\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# Write params.json\n",
    "# -----------------------------------------------------\n",
    "params = {\n",
    "    \"detectors\": detectors,\n",
    "    \"bands\": bands\n",
    "}\n",
    "\n",
    "params_path = os.path.join(USERCONF_DIR, \"params.json\")\n",
    "\n",
    "with open(params_path, \"w\") as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"params.json written to:\", params_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbe7bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN SEN2VM (BUILD + RUN + CLEAN) ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "# =====================================================\n",
    "# Resolve paths\n",
    "# =====================================================\n",
    "\n",
    "dockerfile_dir = os.path.join(root_dir, \"src\", \"sen2vm-core\")\n",
    "\n",
    "config_inside = \"/workspace/UserConf/config.json\"\n",
    "params_inside = \"/workspace/UserConf/params.json\"\n",
    "\n",
    "UID = str(os.getuid())\n",
    "GID = str(os.getgid())\n",
    "\n",
    "# =====================================================\n",
    "# 1. BUILD DOCKER IMAGE\n",
    "# =====================================================\n",
    "\n",
    "print(f\"Building Docker image 'sen2vm' from: {dockerfile_dir}\")\n",
    "\n",
    "cmd_build = [\n",
    "    \"docker\", \"build\",\n",
    "    \"-t\", \"sen2vm\",\n",
    "    dockerfile_dir\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_build), \"\\n\")\n",
    "subprocess.run(cmd_build, check=True)\n",
    "print(\"Docker image built successfully.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. RUN SEN2VM CONTAINER\n",
    "# =====================================================\n",
    "\n",
    "cmd_run = [\n",
    "    \"docker\", \"run\",\n",
    "    \"--rm\",\n",
    "    \"-v\", f\"{root_dir}:/workspace\",\n",
    "    \"sen2vm\",\n",
    "    \"-c\", config_inside,\n",
    "    \"-p\", params_inside\n",
    "]\n",
    "\n",
    "print(\"Running Docker container...\\n\")\n",
    "print(\"Command:\", \" \".join(cmd_run), \"\\n\")\n",
    "\n",
    "subprocess.run(cmd_run, check=True)\n",
    "\n",
    "print(\"\\nDocker execution complete.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. REMOVE DOCKER IMAGE\n",
    "# =====================================================\n",
    "\n",
    "print(\"Removing Docker image 'sen2vm'...\")\n",
    "\n",
    "subprocess.run([\"docker\", \"rmi\", \"-f\", \"sen2vm\"], check=True)\n",
    "\n",
    "print(\"Docker image removed.\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2189b6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "root_dir = os.path.dirname(os.getcwd())\n",
    "\n",
    "# Locate the correct L1B MTD XML\n",
    "xml_path = None\n",
    "for f in os.listdir(PATH_L1B_DATA):\n",
    "    if f.endswith(\".xml\") and \"MTD\" in f:\n",
    "        xml_path = os.path.join(PATH_L1B_DATA, f)\n",
    "        break\n",
    "\n",
    "if xml_path is None:\n",
    "    raise RuntimeError(\"Cannot find L1B MTD XML inside PATH_L1B_DATA.\")\n",
    "\n",
    "print(\"Using XML:\", xml_path)\n",
    "\n",
    "xml_docker = \"/workspace/\" + xml_path.replace(root_dir + \"/\", \"\")\n",
    "\n",
    "# =======================================\n",
    "# GDAL user parameters\n",
    "# =======================================\n",
    "\n",
    "epsg_code = UTM_EPSG\n",
    "\n",
    "ulx = LOCATION[\"ul_x\"]\n",
    "uly = LOCATION[\"ul_y\"]\n",
    "lrx = LOCATION[\"lr_x\"]\n",
    "lry = LOCATION[\"lr_y\"]\n",
    "\n",
    "# =======================================\n",
    "# Output folder: /DATA/GDAL_OUTPUT\n",
    "# =======================================\n",
    "OUTDIR = os.path.join(root_dir, \"DATA\", \"GDAL_OUTPUT\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "OUTDIR_DOCKER = \"/workspace/DATA/GDAL_OUTPUT\"\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 1. Build GDAL docker\n",
    "# =======================================\n",
    "dockerfile_dir = os.path.join(root_dir, \"src\", \"gdal-latest\")\n",
    "\n",
    "print(\"\\n=== BUILDING GDAL LATEST CONTAINER ===\\n\")\n",
    "\n",
    "cmd_build = [\n",
    "    \"docker\", \"build\",\n",
    "    \"--platform=linux/amd64\",\n",
    "    \"-t\", \"gdal-latest\",\n",
    "    dockerfile_dir\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_build), \"\\n\")\n",
    "subprocess.run(cmd_build, check=True)\n",
    "\n",
    "print(\"GDAL image built successfully.\\n\")\n",
    "\n",
    "\n",
    "# ================================\n",
    "# Generate script: gdal_ortho.sh\n",
    "# ================================\n",
    "\n",
    "gdal_script_path = os.path.join(root_dir, \"src\", \"gdal_ortho.sh\")\n",
    "\n",
    "bands_str = \" \".join(ORTHO_SETTINGS[\"keep_bands\"])\n",
    "bands_array = \" \".join([f'\"{b}\"' for b in ORTHO_SETTINGS[\"keep_bands\"]])\n",
    "\n",
    "with open(gdal_script_path, \"w\") as f:\n",
    "    f.write(f\"\"\"#!/bin/bash\n",
    "\n",
    "XML=\"{xml_docker}\"\n",
    "OUT_ORTHO=\"/workspace/DATA/GDAL_OUTPUT\"\n",
    "OUT_MOSAIC=\"/workspace/DATA/GDAL_MOSAIC\"\n",
    "\n",
    "mkdir -p \"$OUT_ORTHO\"\n",
    "mkdir -p \"$OUT_MOSAIC\"\n",
    "\n",
    "KEEP_BANDS=\"{bands_str}\"\n",
    "SDS_LIST_FILE=\"$OUT_ORTHO/sds_list.txt\"\n",
    "rm -f \"$SDS_LIST_FILE\"\n",
    "\n",
    "echo \"[1/2] Listing subdatasets...\"\n",
    "SDS_ALL=($(gdalinfo \"$XML\" | grep SUBDATASET_ | grep NAME= | cut -d= -f2))\n",
    "\n",
    "echo \"-> ${{#SDS_ALL[@]}} subdatasets found\"\n",
    "echo \"\"\n",
    "\n",
    "for SDS in \"${{SDS_ALL[@]}}\"; do\n",
    "    echo \"$SDS\" >> \"$SDS_LIST_FILE\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== ORTHORECTIFICATION ===\"\n",
    "echo \"\"\n",
    "\n",
    "for SDS in \"${{SDS_ALL[@]}}\"; do\n",
    "\n",
    "    NAME=$(echo \"$SDS\" | sed 's/.*://')\n",
    "    BAND=$(echo \"$NAME\" | grep -o \"B[0-9A]\\\\+\")\n",
    "\n",
    "    if [[ ! \" $KEEP_BANDS \" =~ \" $BAND \" ]]; then\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    BASENAME=$(basename \"$NAME\")\n",
    "    OUT=\"${{OUT_ORTHO}}/${{BASENAME}}_ortho.tif\"\n",
    "\n",
    "    FULL_SDS=SENTINEL2_L1B_WITH_GEOLOC:\"$XML\":$NAME\n",
    "\n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Processing: $NAME  (Band = $BAND)\"\n",
    "    echo \"----------------------------------------\"\n",
    "\n",
    "    rm -f \"$OUT\"\n",
    "\n",
    "    gdalwarp \"$FULL_SDS\" \"$OUT\" \\\n",
    "        -t_srs EPSG:{epsg_code} \\\n",
    "        -tr {ORTHO_SETTINGS[\"global_resolution\"]} {ORTHO_SETTINGS[\"global_resolution\"]} \\\n",
    "        -te {ulx} {lry} {lrx} {uly} \\\n",
    "        -r bilinear \\\n",
    "        -co COMPRESS=LZW \\\n",
    "        -co TILED=YES \\\n",
    "        -overwrite\n",
    "\n",
    "    echo \"\"\n",
    "done\n",
    "\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== MOSAIC GENERATION ===\"\n",
    "echo \"\"\n",
    "\n",
    "for BAND in {bands_array}; do\n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Creating mosaic for band: $BAND\"\n",
    "    echo \"----------------------------------------\"\n",
    "\n",
    "    INPUT_FILES=($(ls ${{OUT_ORTHO}}/*_${{BAND}}_ortho.tif 2>/dev/null))\n",
    "\n",
    "    if [ ${{#INPUT_FILES[@]}} -eq 0 ]; then\n",
    "        echo \"No ortho images found for band $BAND\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    OUTPUT=\"${{OUT_MOSAIC}}/ORTHO_mosaic_${{BAND}}.tif\"\n",
    "\n",
    "    gdalwarp \\\n",
    "        \"${{INPUT_FILES[@]}}\" \\\n",
    "        \"$OUTPUT\" \\\n",
    "        -r bilinear \\\n",
    "        -dstnodata 0 \\\n",
    "        -srcnodata 0 \\\n",
    "        -multi \\\n",
    "        -wm 2048 \\\n",
    "        -overwrite \\\n",
    "        -co COMPRESS=LZW \\\n",
    "        -co TILED=YES \\\n",
    "        -ot UInt16\n",
    "\n",
    "    echo \" Mosaic written → $OUTPUT\"\n",
    "    echo \"\"\n",
    "done\n",
    "\n",
    "echo \"=== GDAL processing complete ===\"\n",
    "\"\"\")\n",
    "\n",
    "os.chmod(gdal_script_path, 0o755)\n",
    "\n",
    "print(\"Generated:\", gdal_script_path)\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 3. Run GDAL processing inside container\n",
    "# =======================================\n",
    "\n",
    "print(\"\\n=== RUNNING GDAL PROCESSING ===\\n\")\n",
    "\n",
    "cmd_run = [\n",
    "    \"docker\", \"run\",\n",
    "    \"--rm\",\n",
    "    \"-v\", f\"{root_dir}:/workspace\",\n",
    "    \"gdal-latest\",\n",
    "    \"/workspace/src/gdal_ortho.sh\"\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_run), \"\\n\")\n",
    "subprocess.run(cmd_run, check=True)\n",
    "\n",
    "print(\"\\nGDAL ortho + mosaic complete.\\n\")\n",
    "\n",
    "\n",
    "# =======================================\n",
    "# 4. Delete GDAL docker image\n",
    "# =======================================\n",
    "print(\"Removing gdal-latest image...\\n\")\n",
    "\n",
    "subprocess.run([\"docker\", \"rmi\", \"-f\", \"gdal-latest\"], check=True)\n",
    "\n",
    "print(\"GDAL image removed.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
