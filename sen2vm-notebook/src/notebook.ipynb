{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bbfd73",
   "metadata": {},
   "source": [
    "# === USER CONFIGURATION ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === USER CONFIGURATION ===\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, please read the README_Notebooks.md file before\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "WORKDIR = \"PATH/TO/WORKDIR\"  # Working directory for sen2vm processing\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, the structure of the WORKDIR shall respect the one described in README_Notebooks.md\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "# Path to downloaded product\n",
    "PATH_L1B_DATA = \"PATH/TO/L1B/PRODUCT\"\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, for now PATH_L1B_DATA shall be inside WORKID/DATA and respect the one described in README_Notebooks.md\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "# Output folder chosen by the user\n",
    "INVERSE_OUTPUT_FOLDER = \"PATH/TO/OUTPUT/FOLDER\"  # Used only if GRID_MODE = \"inverse\"\n",
    "\n",
    "\n",
    "# === SEN2VM OPTIONS ===\n",
    "\n",
    "GRID_MODE = \"direct\"  # direct or inverse\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, for now only direct shall be used with a full product (no missing granules)\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "UTM_EPSG = 23037 # UTM zone EPSG code for the ROI\n",
    "LOCATION =  {\n",
    "    \"ul_x\": -185299,\n",
    "    \"ul_y\": 3363178,\n",
    "    \"lr_x\": -17915,\n",
    "    \"lr_y\": 3352896\n",
    "}\n",
    "\n",
    "STEPS = {\n",
    "    \"10m_bands\": 10,\n",
    "    \"20m_bands\": 20,\n",
    "    \"60m_bands\": 60\n",
    "}\n",
    "\n",
    "# === GDAL ORTHO OPTIONS ===\n",
    "\n",
    "DELETE_TMP_FILES = True  # Delete temporary files created during orthorectification\n",
    "\n",
    "ORTHO_SETTINGS = {\n",
    "    \"keep_bands\": [\"B01\" , \"B02\", \"B03\", \"B04\", \"B05\", \"B06\", \"B07\", \"B08\", \"B8A\", \"B09\", \"B10\", \"B11\", \"B12\"], # list of bands to keep for orthorectification\n",
    "}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e95184",
   "metadata": {},
   "source": [
    "# GIPP database download (Optionnal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GIPPs ===\n",
    "# This step is optionnal if the Database was already downloaded or if you want to use your own GIPP\n",
    "# Please note that this current notebook will search for a subfolder with mission S2[A/B/C] inside the GIPP folder\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "gipp_dir = os.path.join(WORKDIR, \"DATA\")     \n",
    "os.makedirs(gipp_dir, exist_ok=True)             \n",
    "\n",
    "# =====================================================================\n",
    "# 1) CLONE sen2vm-gipp-database\n",
    "# =====================================================================\n",
    "\n",
    "gipp_repo_name = \"sen2vm-gipp-database\"\n",
    "gipp_repo_path = os.path.join(gipp_dir, gipp_repo_name)\n",
    "\n",
    "# Delete repository if exists\n",
    "if os.path.exists(gipp_repo_path):\n",
    "    print(f\"Removing existing repository: {gipp_repo_path}\")\n",
    "    shutil.rmtree(gipp_repo_path)\n",
    "\n",
    "# Clone repository fresh\n",
    "print(\"Cloning sen2vm-gipp-database...\")\n",
    "!git clone https://github.com/sen2vm/sen2vm-gipp-database.git {gipp_repo_path}\n",
    "print(\"Clone complete.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"GIPP processing finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f616a3",
   "metadata": {},
   "source": [
    "# IERS Download (optionnal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD IERS ===\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = os.path.join(WORKDIR, \"DATA\")\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise RuntimeError(f\"DATA directory not found: {DATA_DIR}\")\n",
    "\n",
    "print(\"Bulletin output directory:\", DATA_DIR)\n",
    "\n",
    "# =====================================================================\n",
    "# 0. REMOVE EXISTING IERS BULLETINS\n",
    "# =====================================================================\n",
    "\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.startswith(\"bulletina-\") and f.endswith(\".txt\"):\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        print(\"Removed old bulletin A:\", f)\n",
    "\n",
    "    if f.startswith(\"bulletinb-\") and f.endswith(\".txt\"):\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        print(\"Removed old bulletin B:\", f)\n",
    "\n",
    "print(\"Cleanup of old bulletins complete.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2. EXTRACT PRODUCT DATE (FROM DATASTRIP)\n",
    "# =====================================================================\n",
    "\n",
    "datastrip_dir = os.path.join(PATH_L1B_DATA, \"DATASTRIP\")\n",
    "\n",
    "if not os.path.isdir(datastrip_dir):\n",
    "    raise RuntimeError(f\"DATASTRIP directory not found: {datastrip_dir}\")\n",
    "\n",
    "datastrip_entries = os.listdir(datastrip_dir)\n",
    "if not datastrip_entries:\n",
    "    raise RuntimeError(f\"No DATASTRIP found in: {datastrip_dir}\")\n",
    "\n",
    "# Take the first DATASTRIP product\n",
    "datastrip_name = datastrip_entries[0]\n",
    "\n",
    "match = re.search(r\"_S(\\d{8})T\\d{6}_\", datastrip_name)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Could not extract product date from DATASTRIP name.\")\n",
    "\n",
    "product_date_str = match.group(1)\n",
    "\n",
    "year = int(product_date_str[:4])\n",
    "month = int(product_date_str[4:6])\n",
    "day = int(product_date_str[6:8])\n",
    "\n",
    "product_date = datetime(year, month, day)\n",
    "\n",
    "print(\"Product date extracted from DATASTRIP:\", product_date.date(), \"\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# Roman conversion \n",
    "# =====================================================================\n",
    "\n",
    "def int_to_roman(n):\n",
    "    vals = [\n",
    "        (1000, 'm'), (900, 'cm'), (500, 'd'), (400, 'cd'),\n",
    "        (100, 'c'), (90, 'xc'), (50, 'l'), (40, 'xl'),\n",
    "        (10, 'x'), (9, 'ix'), (5, 'v'), (4, 'iv'), (1, 'i')\n",
    "    ]\n",
    "    res = \"\"\n",
    "    for v, s in vals:\n",
    "        while n >= v:\n",
    "            res += s\n",
    "            n -= v\n",
    "    return res\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Bulletin A\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "roman_year = int_to_roman(year - 1987)\n",
    "doy = product_date.timetuple().tm_yday\n",
    "index = (doy - 1) // 7 + 1\n",
    "\n",
    "print(f\"Bulletin A Roman year: {roman_year}\")\n",
    "print(f\"Initial weekly index: {index}\\n\")\n",
    "\n",
    "found = False\n",
    "\n",
    "while index > 0:\n",
    "    index_str = f\"{index:03d}\"\n",
    "    url = f\"https://datacenter.iers.org/data/6/bulletina-{roman_year}-{index_str}.txt\"\n",
    "    print(\"Trying bulletin:\", url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Bulletin found:\", index_str)\n",
    "        dest_file = os.path.join(DATA_DIR, f\"bulletina-{roman_year}-{index_str}.txt\")\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "    index -= 1\n",
    "\n",
    "if not found:\n",
    "    raise RuntimeError(\"No Bulletin A available for current or previous weeks.\")\n",
    "\n",
    "with open(dest_file, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"Downloaded:\", dest_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85608d",
   "metadata": {},
   "source": [
    "# Sen2VM configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276592b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE CONFIG.JSON  ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from numpy import double\n",
    "\n",
    "\n",
    "USERCONF_DIR = os.path.join(WORKDIR, \"UserConf\")\n",
    "os.makedirs(USERCONF_DIR, exist_ok=True)\n",
    "\n",
    "print(\"UserConf directory:\", USERCONF_DIR)\n",
    "\n",
    "GEOID_DIR = os.path.join(WORKDIR, \"DATA\", \"GEOID\")\n",
    "os.makedirs(GEOID_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Geoid directory:\", GEOID_DIR)\n",
    "\n",
    "# =====================================================\n",
    "# 1. Extract mission from DATASTRIP\n",
    "# =====================================================\n",
    "\n",
    "datastrip_dir = os.path.join(PATH_L1B_DATA, \"DATASTRIP\")\n",
    "\n",
    "if not os.path.isdir(datastrip_dir):\n",
    "    raise RuntimeError(f\"DATASTRIP directory not found: {datastrip_dir}\")\n",
    "\n",
    "datastrip_entries = os.listdir(datastrip_dir)\n",
    "if not datastrip_entries:\n",
    "    raise RuntimeError(f\"No DATASTRIP found in: {datastrip_dir}\")\n",
    "\n",
    "datastrip_name = datastrip_entries[0]\n",
    "\n",
    "match = re.match(r\"(S2[A-C])_OPER_\", datastrip_name)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Cannot extract mission (S2A/S2B/S2C) from DATASTRIP name.\")\n",
    "\n",
    "mission = match.group(1)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. Docker paths inside /workspace\n",
    "# =====================================================\n",
    "\n",
    "safe_name = os.path.basename(PATH_L1B_DATA)\n",
    "\n",
    "docker_l1b  = f\"/workspace/DATA/{safe_name}\"\n",
    "docker_dem  = \"/workspace/DATA/DEM\"\n",
    "docker_gipp = f\"/workspace/DATA/sen2vm-gipp-database/{mission}\"\n",
    "\n",
    "# =====================================================\n",
    "# 3. Geoid management\n",
    "# =====================================================\n",
    "\n",
    "# Notebook location (NOT relative to CWD)\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Relative path to DEM_GEOID from notebook\n",
    "internal_geoid_dir = os.path.abspath(os.path.join(\n",
    "    notebook_dir,\n",
    "    \"..\", \"..\", \"src\", \"test\", \"resources\", \"DEM_GEOID\"\n",
    "))\n",
    "\n",
    "# If WORKDIR/DATA/GEOID is empty -> copy internal files\n",
    "if len(os.listdir(GEOID_DIR)) == 0:\n",
    "    print(\"GEOID directory is empty -> copying default geoid files...\")\n",
    "    for f in os.listdir(internal_geoid_dir):\n",
    "        src = os.path.join(internal_geoid_dir, f)\n",
    "        dst = os.path.join(GEOID_DIR, f)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Detect .gtx inside GEOID_DIR\n",
    "geoid_files = [f for f in os.listdir(GEOID_DIR) if f.lower().endswith(\".gtx\")]\n",
    "if len(geoid_files) == 0:\n",
    "    raise RuntimeError(\"No .gtx geoid file found in WORKDIR/DATA/GEOID.\")\n",
    "\n",
    "docker_geoid = f\"/workspace/DATA/GEOID/{geoid_files[0]}\"\n",
    "\n",
    "# =====================================================\n",
    "# 4. Locate IERS bulletin on host\n",
    "# =====================================================\n",
    "\n",
    "DATA_DIR = os.path.join(WORKDIR, \"DATA\")\n",
    "iers_host = None\n",
    "\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.startswith(\"bulletin\"):\n",
    "        iers_host = os.path.join(DATA_DIR, f)\n",
    "        break\n",
    "\n",
    "if iers_host is None:\n",
    "    raise RuntimeError(\"IERS bulletin not found inside WORKDIR/DATA directory.\")\n",
    "\n",
    "docker_iers = \"/workspace/DATA/\" + os.path.basename(iers_host)\n",
    "\n",
    "# =====================================================\n",
    "# 5. Build config dictionary\n",
    "# =====================================================\n",
    "\n",
    "config = {\n",
    "    \"l1b_product\": docker_l1b,\n",
    "    \"gipp_folder\": docker_gipp,\n",
    "    \"auto_gipp_selection\": True,\n",
    "    \"grids_overwriting\": True,\n",
    "    \"dem\": docker_dem,\n",
    "    \"geoid\": docker_geoid,\n",
    "    \"iers\": docker_iers,\n",
    "    \"operation\": GRID_MODE,\n",
    "    \"deactivate_available_refining\": False,\n",
    "    \"steps\": {\n",
    "        \"10m_bands\": STEPS[\"10m_bands\"],\n",
    "        \"20m_bands\": STEPS[\"20m_bands\"],\n",
    "        \"60m_bands\": STEPS[\"60m_bands\"]\n",
    "    },\n",
    "    \"export_alt\": True\n",
    "}\n",
    "\n",
    "# Add inverse block only if needed\n",
    "if GRID_MODE == \"inverse\":\n",
    "    config[\"inverse_location_additional_info\"] = {\n",
    "        \"ul_x\": double(LOCATION[\"ul_x\"]),\n",
    "        \"ul_y\": double(LOCATION[\"ul_y\"]),\n",
    "        \"lr_x\": double(LOCATION[\"lr_x\"]),\n",
    "        \"lr_y\": double(LOCATION[\"lr_y\"]),\n",
    "        \"referential\": f\"EPSG:{UTM_EPSG}\",\n",
    "        \"output_folder\": \"/workspace/DATA/Output\"\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# Save config.json\n",
    "# =====================================================\n",
    "\n",
    "config_path = os.path.join(USERCONF_DIR, \"config.json\")\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"Configuration file generated:\")\n",
    "print(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE PARAMS.JSON ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "USERCONF_DIR = os.path.join(WORKDIR, \"UserConf\")\n",
    "os.makedirs(USERCONF_DIR, exist_ok=True)\n",
    "\n",
    "print(\"UserConf directory:\", USERCONF_DIR)\n",
    "\n",
    "# =====================================================\n",
    "# Locate GRANULE folders\n",
    "# =====================================================\n",
    "GR_TARGET_DIR = os.path.join(PATH_L1B_DATA, \"GRANULE\")\n",
    "\n",
    "if not os.path.exists(GR_TARGET_DIR):\n",
    "    raise RuntimeError(\"GRANULE directory not found inside L1B SAFE.\")\n",
    "\n",
    "granule_folders = [\n",
    "    os.path.join(GR_TARGET_DIR, d)\n",
    "    for d in os.listdir(GR_TARGET_DIR)\n",
    "    if os.path.isdir(os.path.join(GR_TARGET_DIR, d))\n",
    "]\n",
    "\n",
    "print(\"Found\", len(granule_folders), \"granule folders.\")\n",
    "\n",
    "# =====================================================\n",
    "# Extract detectors and bands from JP2\n",
    "# =====================================================\n",
    "detectors = set()\n",
    "bands = set()\n",
    "\n",
    "pattern = r\"_D(\\d+)_B(\\d{1,2}[A]?)\\.jp2$\"\n",
    "\n",
    "for granule in granule_folders:\n",
    "    img_data_dir = os.path.join(granule, \"IMG_DATA\")\n",
    "\n",
    "    if not os.path.isdir(img_data_dir):\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(img_data_dir):\n",
    "        match = re.search(pattern, fname)\n",
    "        if match:\n",
    "            detectors.add(match.group(1))\n",
    "            bands.add(f\"B{match.group(2)}\")\n",
    "\n",
    "detectors = sorted(detectors)\n",
    "bands = sorted(bands)\n",
    "\n",
    "print(\"Detected detectors:\", detectors)\n",
    "print(\"Detected bands:\", bands)\n",
    "\n",
    "# =====================================================\n",
    "# Write params.json\n",
    "# =====================================================\n",
    "params = {\n",
    "    \"detectors\": detectors,\n",
    "    \"bands\": bands\n",
    "}\n",
    "\n",
    "params_path = os.path.join(USERCONF_DIR, \"params.json\")\n",
    "\n",
    "with open(params_path, \"w\") as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"params.json written to:\", params_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f67fe",
   "metadata": {},
   "source": [
    "# Sen2VM run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN SEN2VM (Docker: BUILD + RUN + CLEAN) ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "dockerfile_dir = os.path.abspath(os.path.join(\n",
    "    notebook_dir,\n",
    "    \"..\", \"..\"\n",
    "))\n",
    "\n",
    "config_inside = \"/workspace/UserConf/config.json\"\n",
    "params_inside = \"/workspace/UserConf/params.json\"\n",
    "\n",
    "# =====================================================\n",
    "# 1. BUILD DOCKER IMAGE\n",
    "# =====================================================\n",
    "\n",
    "print(f\"Building Docker image 'sen2vm' from: {dockerfile_dir}\")\n",
    "\n",
    "cmd_build = [\n",
    "    \"docker\", \"build\",\n",
    "    \"-t\", \"sen2vm\",\n",
    "    dockerfile_dir\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_build), \"\\n\")\n",
    "subprocess.run(cmd_build, check=True)\n",
    "print(\"Docker image built successfully.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. RUN SEN2VM CONTAINER\n",
    "# =====================================================\n",
    "\n",
    "cmd_run = [\n",
    "    \"docker\", \"run\",\n",
    "    \"--rm\",\n",
    "    \"-v\", f\"{WORKDIR}:/workspace\",  \n",
    "    \"sen2vm\",\n",
    "    \"-c\", config_inside,\n",
    "    \"-p\", params_inside\n",
    "]\n",
    "\n",
    "print(\"Running Docker container...\\n\")\n",
    "print(\"Command:\", \" \".join(cmd_run), \"\\n\")\n",
    "\n",
    "subprocess.run(cmd_run, check=True)\n",
    "\n",
    "print(\"\\nDocker execution complete.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. REMOVE DOCKER IMAGE\n",
    "# =====================================================\n",
    "\n",
    "print(\"Removing Docker image 'sen2vm'...\")\n",
    "\n",
    "subprocess.run([\"docker\", \"rmi\", \"-f\", \"sen2vm\"], check=True)\n",
    "\n",
    "print(\"Docker image removed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb33ec9",
   "metadata": {},
   "source": [
    "# TEST IF THE PRODUCT IS COMPLET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebbe2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import sys\n",
    "import xml.etree.ElementTree as ET\n",
    "from collections import defaultdict\n",
    "from typing import Set, List, Optional\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Common helpers\n",
    "# =====================================================\n",
    "def _abspath(path: str) -> str:\n",
    "    return os.path.abspath(os.path.expanduser(path))\n",
    "\n",
    "\n",
    "def print_list(title: str, items: List[str], limit: Optional[int] = None) -> None:\n",
    "    print(title)\n",
    "    if not items:\n",
    "        print(\"  (none)\")\n",
    "        return\n",
    "\n",
    "    if limit is not None and limit > 0 and len(items) > limit:\n",
    "        for it in items[:limit]:\n",
    "            print(\"  -\", it)\n",
    "        print(f\"  ... ({len(items) - limit} more)\")\n",
    "        return\n",
    "\n",
    "    for it in items:\n",
    "        print(\"  -\", it)\n",
    "\n",
    "\n",
    "def find_datastrip_xml(safe_path: str) -> str:\n",
    "    patterns = [\n",
    "        os.path.join(safe_path, \"DATASTRIP\", \"S2*\", \"S2*_MTD_L1B_DS_*.xml\"),\n",
    "        os.path.join(safe_path, \"DATASTRIP\", \"S2*\", \"MTD_L1B_DS_*.xml\"),\n",
    "    ]\n",
    "    xmls: List[str] = []\n",
    "    for p in patterns:\n",
    "        xmls.extend(glob.glob(p))\n",
    "    xmls = sorted(set(xmls))\n",
    "    if not xmls:\n",
    "        raise FileNotFoundError(f\"No DATASTRIP MTD XML found under: {safe_path}/DATASTRIP\")\n",
    "    if len(xmls) > 1:\n",
    "        print(\"WARNING: Multiple DATASTRIP XML found; using the first one:\", file=sys.stderr)\n",
    "        for x in xmls:\n",
    "            print(\"  -\", x, file=sys.stderr)\n",
    "    return xmls[0]\n",
    "\n",
    "\n",
    "def list_granule_dirs(safe_path: str) -> Set[str]:\n",
    "    granule_root = os.path.join(safe_path, \"GRANULE\")\n",
    "    if not os.path.isdir(granule_root):\n",
    "        raise FileNotFoundError(f\"Missing GRANULE directory: {granule_root}\")\n",
    "    return {e for e in os.listdir(granule_root) if os.path.isdir(os.path.join(granule_root, e))}\n",
    "\n",
    "\n",
    "def extract_expected_granules_from_xml(xml_path: str) -> Set[str]:\n",
    "    expected: Set[str] = set()\n",
    "\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    for elem in root.iter():\n",
    "        if elem.text:\n",
    "            text = elem.text.strip()\n",
    "            if text.startswith(\"S2\") and \"_GR_\" in text:\n",
    "                expected.add(text)\n",
    "\n",
    "        for val in elem.attrib.values():\n",
    "            if val.startswith(\"S2\") and \"_GR_\" in val:\n",
    "                expected.add(val)\n",
    "\n",
    "    if not expected:\n",
    "        print(\"WARNING: No granule identifiers could be extracted from XML.\\n\", file=sys.stderr)\n",
    "\n",
    "    return expected\n",
    "\n",
    "\n",
    "def check_img_data_jp2(safe_path: str, granules: Set[str]) -> List[str]:\n",
    "    problems: List[str] = []\n",
    "    for g in sorted(granules):\n",
    "        img_dir = os.path.join(safe_path, \"GRANULE\", g, \"IMG_DATA\")\n",
    "        if not os.path.isdir(img_dir):\n",
    "            problems.append(f\"{g} -> IMG_DATA directory missing\")\n",
    "            continue\n",
    "        if not glob.glob(os.path.join(img_dir, \"*.jp2\")):\n",
    "            problems.append(f\"{g} -> no *.jp2 found in IMG_DATA\")\n",
    "    return problems\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Completeness check\n",
    "# =====================================================\n",
    "def is_complete(\n",
    "    safe_path: str,\n",
    "    check_img: bool = False,\n",
    "    list_limit: int = 50,\n",
    ") -> bool:\n",
    "    safe_path = _abspath(safe_path)\n",
    "\n",
    "    if not os.path.isdir(safe_path):\n",
    "        print(f\"ERROR: Not a directory: {safe_path}\", file=sys.stderr)\n",
    "        return False\n",
    "\n",
    "    xml_path = find_datastrip_xml(safe_path)\n",
    "    actual = list_granule_dirs(safe_path)\n",
    "    expected = extract_expected_granules_from_xml(xml_path)\n",
    "\n",
    "    present_in_both = sorted(actual & expected) if expected else []\n",
    "    missing = sorted(expected - actual) if expected else []\n",
    "    extra = sorted(actual - expected) if expected else []\n",
    "\n",
    "    print(\"SAFE:\", safe_path)\n",
    "    print(\"DATASTRIP XML:\", xml_path)\n",
    "    print(\"Actual GRANULE count:\", len(actual))\n",
    "    print(\"Expected GRANULE count (from XML):\", len(expected))\n",
    "    print(\"Present in BOTH (disk âˆ© XML) count:\", len(present_in_both))\n",
    "    print(\"Missing (XML - disk) count:\", len(missing))\n",
    "    print(\"Extra (disk - XML) count:\", len(extra))\n",
    "    print(\"\")\n",
    "\n",
    "    if expected:\n",
    "        print_list(\"PRESENT IN BOTH (referenced by XML AND present on disk):\", present_in_both, limit=list_limit)\n",
    "        print(\"\")\n",
    "        print_list(\"MISSING GRANULES (referenced by XML but NOT present on disk):\", missing, limit=list_limit)\n",
    "        print(\"\")\n",
    "        print_list(\"EXTRA GRANULES (present on disk but NOT referenced by XML):\", extra, limit=list_limit)\n",
    "        print(\"\")\n",
    "    else:\n",
    "        print(\"Skipping XML vs disk comparison (no expected granules extracted).\")\n",
    "        print(\"\")\n",
    "\n",
    "    img_problems: List[str] = []\n",
    "    if check_img:\n",
    "        to_check = set(present_in_both) if expected else actual\n",
    "        img_problems = check_img_data_jp2(safe_path, to_check)\n",
    "\n",
    "        if img_problems:\n",
    "            print(\"IMG_DATA CHECK PROBLEMS:\")\n",
    "            for p in img_problems[:list_limit]:\n",
    "                print(\"  -\", p)\n",
    "            if len(img_problems) > list_limit:\n",
    "                print(f\"  ... ({len(img_problems) - list_limit} more)\")\n",
    "            print(\"\")\n",
    "        else:\n",
    "            print(\"IMG_DATA CHECK: OK\")\n",
    "            print(\"\")\n",
    "\n",
    "    return (len(missing) == 0) and (len(extra) == 0) and (len(img_problems) == 0)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# TMP workdir copy only if incomplete product\n",
    "# =====================================================\n",
    "def ensure_workdir_tmp_and_copy_product(\n",
    "    workdir: str,\n",
    "    path_l1b_data: str,\n",
    "    suffix: str = \"_tmp\",\n",
    ") -> tuple[str, str]:\n",
    "    workdir = _abspath(workdir)\n",
    "    path_l1b_data = _abspath(path_l1b_data)\n",
    "\n",
    "    workdir_parent = os.path.dirname(workdir)\n",
    "    workdir_base = os.path.basename(workdir.rstrip(os.sep))\n",
    "    workdir_tmp = os.path.join(workdir_parent, f\"{workdir_base}{suffix}\")\n",
    "\n",
    "    if not path_l1b_data.startswith(workdir + os.sep):\n",
    "        raise ValueError(\n",
    "            \"PATH_L1B_DATA must be located inside WORKDIR.\\n\"\n",
    "            f\"WORKDIR      : {workdir}\\n\"\n",
    "            f\"PATH_L1B_DATA : {path_l1b_data}\"\n",
    "        )\n",
    "\n",
    "    rel_product_path = os.path.relpath(path_l1b_data, start=workdir)\n",
    "    path_l1b_data_tmp = os.path.join(workdir_tmp, rel_product_path)\n",
    "\n",
    "    os.makedirs(workdir_tmp, exist_ok=True)\n",
    "    os.makedirs(os.path.dirname(path_l1b_data_tmp), exist_ok=True)\n",
    "\n",
    "    if not os.path.exists(path_l1b_data_tmp):\n",
    "        print(f\"Copying WORKDIR -> WORKDIR_TMP:\\n  {workdir}\\n  -> {workdir_tmp}\")\n",
    "        shutil.copytree(workdir, workdir_tmp, dirs_exist_ok=True)\n",
    "    else:\n",
    "        print(f\"WORKDIR_TMP already exists, not copying again:\\n  {workdir_tmp}\")\n",
    "\n",
    "    return workdir_tmp, path_l1b_data_tmp\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# XML trim only if incomplete product\n",
    "# =====================================================\n",
    "def _extract_granule_ids_from_elem(elem: ET.Element) -> Set[str]:\n",
    "    ids: Set[str] = set()\n",
    "    if elem.text:\n",
    "        t = elem.text.strip()\n",
    "        if t.startswith(\"S2\") and \"_GR_\" in t:\n",
    "            ids.add(t)\n",
    "    for v in elem.attrib.values():\n",
    "        if v.startswith(\"S2\") and \"_GR_\" in v:\n",
    "            ids.add(v)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _extract_granule_ids_from_subtree(elem: ET.Element) -> Set[str]:\n",
    "    ids: Set[str] = set()\n",
    "    for e in elem.iter():\n",
    "        ids |= _extract_granule_ids_from_elem(e)\n",
    "    return ids\n",
    "\n",
    "\n",
    "def _build_parent_map(root: ET.Element) -> dict[int, ET.Element]:\n",
    "    parent_map: dict[int, ET.Element] = {}\n",
    "    for parent in root.iter():\n",
    "        for child in list(parent):\n",
    "            parent_map[id(child)] = parent\n",
    "    return parent_map\n",
    "\n",
    "\n",
    "def trim_datastrip_xml_to_existing_granules(\n",
    "    safe_path: str,\n",
    "    xml_path: str,\n",
    ") -> str:\n",
    "    safe_path = _abspath(safe_path)\n",
    "    xml_path = _abspath(xml_path)\n",
    "\n",
    "    if not os.path.isdir(safe_path):\n",
    "        raise FileNotFoundError(f\"SAFE directory not found: {safe_path}\")\n",
    "    if not os.path.isfile(xml_path):\n",
    "        raise FileNotFoundError(f\"DATASTRIP XML not found: {xml_path}\")\n",
    "\n",
    "    actual_granules = list_granule_dirs(safe_path)\n",
    "\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    expected_granules = _extract_granule_ids_from_subtree(root)\n",
    "    keep = actual_granules & expected_granules\n",
    "    drop = expected_granules - keep\n",
    "\n",
    "    print(\"TRIM DATASTRIP XML\")\n",
    "    print(\"SAFE:\", safe_path)\n",
    "    print(\"XML :\", xml_path)\n",
    "    print(\"Actual GRANULE count:\", len(actual_granules))\n",
    "    print(\"Expected GRANULE count (from XML):\", len(expected_granules))\n",
    "    print(\"KEEP (disk & XML) count:\", len(keep))\n",
    "    print(\"DROP (XML - KEEP) count:\", len(drop))\n",
    "\n",
    "    if not keep:\n",
    "        raise RuntimeError(\"KEEP set is empty. Nothing to keep; aborting.\")\n",
    "\n",
    "    parent_map = _build_parent_map(root)\n",
    "    candidates = list(root.iter())[::-1]\n",
    "\n",
    "    removed = 0\n",
    "    for elem in candidates:\n",
    "        if elem is root:\n",
    "            continue\n",
    "\n",
    "        ids_in_elem = _extract_granule_ids_from_subtree(elem)\n",
    "        if not ids_in_elem:\n",
    "            continue\n",
    "        if ids_in_elem & keep:\n",
    "            continue\n",
    "        if ids_in_elem.issubset(drop):\n",
    "            parent = parent_map.get(id(elem))\n",
    "            if parent is None:\n",
    "                continue\n",
    "            try:\n",
    "                parent.remove(elem)\n",
    "                removed += 1\n",
    "            except ValueError:\n",
    "                pass\n",
    "\n",
    "    # Keep SAME output behavior as your original: overwrite input XML path\n",
    "    tree.write(xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "    print(\"Removed XML elements:\", removed)\n",
    "    print(\"Output written:\", xml_path)\n",
    "    print(\"\")\n",
    "\n",
    "    return xml_path\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Position fix only if incomplete product\n",
    "# =====================================================\n",
    "DETECTOR_RE = re.compile(r\"_D(\\d{2})_\")\n",
    "\n",
    "\n",
    "def _localname(tag: str) -> str:\n",
    "    return tag.split(\"}\", 1)[1] if \"}\" in tag else tag\n",
    "\n",
    "\n",
    "def _find_child(elem: ET.Element, child_localname: str) -> Optional[ET.Element]:\n",
    "    for ch in list(elem):\n",
    "        if _localname(ch.tag) == child_localname:\n",
    "            return ch\n",
    "    return None\n",
    "\n",
    "\n",
    "def fix_datastrip_granule_positions(xml_path: str) -> str:\n",
    "    xml_path = _abspath(xml_path)\n",
    "    if not os.path.isfile(xml_path):\n",
    "        raise FileNotFoundError(f\"XML not found: {xml_path}\")\n",
    "\n",
    "    tree = ET.parse(xml_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    granules = [e for e in root.iter() if _localname(e.tag) == \"Granule\"]\n",
    "    if not granules:\n",
    "        raise RuntimeError(\"No <Granule> elements found in this XML.\")\n",
    "\n",
    "    by_det: dict[str, List[tuple[str, int, ET.Element]]] = defaultdict(list)\n",
    "\n",
    "    for g in granules:\n",
    "        gid = g.attrib.get(\"granuleId\", \"\")\n",
    "        m = DETECTOR_RE.search(gid)\n",
    "        det = f\"D{m.group(1)}\" if m else \"UNKNOWN\"\n",
    "\n",
    "        pos_elem = _find_child(g, \"POSITION\")\n",
    "        if pos_elem is None:\n",
    "            continue\n",
    "        try:\n",
    "            pos_val = int((pos_elem.text or \"\").strip())\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        by_det[det].append((gid, pos_val, g))\n",
    "\n",
    "    if not by_det:\n",
    "        raise RuntimeError(\"No granules with <POSITION> found.\")\n",
    "\n",
    "    print(f\"Found {len(granules)} <Granule> elements\")\n",
    "    print(f\"Detectors found: {', '.join(sorted(by_det.keys()))}\")\n",
    "    print(\"\")\n",
    "\n",
    "    for det in sorted(by_det.keys()):\n",
    "        items = by_det[det]\n",
    "        min_pos = min(p for _, p, _ in items)\n",
    "        shift = min_pos - 1\n",
    "        print(f\"{det}: min POSITION={min_pos} -> shift={shift}\")\n",
    "\n",
    "        for gid, pos_val, g in sorted(items, key=lambda x: x[1]):\n",
    "            pos_elem = _find_child(g, \"POSITION\")\n",
    "            if pos_elem is None:\n",
    "                continue\n",
    "            new_pos = pos_val - shift\n",
    "            if new_pos < 1:\n",
    "                print(\n",
    "                    f\"WARNING: {det} would produce POSITION<1 for {gid} (old={pos_val}, new={new_pos})\",\n",
    "                    file=sys.stderr,\n",
    "                )\n",
    "            pos_elem.text = str(new_pos)\n",
    "\n",
    "        print(\"  Example after fix (sorted by old POSITION):\")\n",
    "        for gid, pos_val, _ in sorted(items, key=lambda x: x[1])[:5]:\n",
    "            print(f\"    - {gid} : {pos_val} -> {pos_val - shift}\")\n",
    "        print(\"\")\n",
    "\n",
    "    # Keep SAME output behavior as your original: overwrite input XML path\n",
    "    tree.write(xml_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "\n",
    "    print(f\"Written: {xml_path}\")\n",
    "    print(\"\")\n",
    "\n",
    "    return xml_path\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Main\n",
    "# =====================================================\n",
    "product_is_complete = is_complete(PATH_L1B_DATA, check_img=True, list_limit=200)\n",
    "\n",
    "if product_is_complete:\n",
    "    print(\"L1B product is COMPLETE.\")\n",
    "else:\n",
    "    print(\"L1B product is INCOMPLETE. Creating a tmp copy of WORKDIR...\")\n",
    "\n",
    "    WORKDIR_TMP, PATH_L1B_DATA_TMP = ensure_workdir_tmp_and_copy_product(\n",
    "        workdir=WORKDIR,\n",
    "        path_l1b_data=PATH_L1B_DATA,\n",
    "        suffix=\"_tmp\",\n",
    "    )\n",
    "    print(\"WORKDIR_TMP:\", WORKDIR_TMP)\n",
    "    print(\"PATH_L1B_DATA_TMP:\", PATH_L1B_DATA_TMP)\n",
    "\n",
    "    datastrip_xml_tmp = find_datastrip_xml(PATH_L1B_DATA_TMP)\n",
    "\n",
    "    trimmed_xml_tmp = trim_datastrip_xml_to_existing_granules(\n",
    "        safe_path=PATH_L1B_DATA_TMP,\n",
    "        xml_path=datastrip_xml_tmp,\n",
    "    )\n",
    "    print(\"Trimmed XML path:\", trimmed_xml_tmp)\n",
    "\n",
    "    posfixed_xml_tmp = fix_datastrip_granule_positions(trimmed_xml_tmp)\n",
    "    print(\"Position-fixed XML path:\", posfixed_xml_tmp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54044d2",
   "metadata": {},
   "source": [
    "# Generate Orthorectification images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Choose inputs: normal vs _tmp (and keep outputs in WORKDIR)\n",
    "# =====================================================\n",
    "WORKDIR_ORIG = WORKDIR\n",
    "PATH_L1B_DATA_ORIG = PATH_L1B_DATA\n",
    "\n",
    "# Detect a usable TMP context (created by the previous cell)\n",
    "use_tmp = (\n",
    "    \"WORKDIR_TMP\" in globals()\n",
    "    and \"PATH_L1B_DATA_TMP\" in globals()\n",
    "    and isinstance(globals().get(\"WORKDIR_TMP\"), str)\n",
    "    and isinstance(globals().get(\"PATH_L1B_DATA_TMP\"), str)\n",
    "    and os.path.isdir(globals().get(\"WORKDIR_TMP\"))\n",
    "    and os.path.isdir(globals().get(\"PATH_L1B_DATA_TMP\"))\n",
    ")\n",
    "\n",
    "# Switch inputs if tmp exists, but keep outputs in the original WORKDIR\n",
    "WORKDIR_INPUT = globals().get(\"WORKDIR_TMP\") if use_tmp else WORKDIR_ORIG\n",
    "PATH_L1B_DATA_INPUT = globals().get(\"PATH_L1B_DATA_TMP\") if use_tmp else PATH_L1B_DATA_ORIG\n",
    "\n",
    "print(\"use_tmp:\", use_tmp)\n",
    "print(\"INPUT WORKDIR:\", WORKDIR_INPUT)\n",
    "print(\"INPUT PATH_L1B_DATA:\", PATH_L1B_DATA_INPUT)\n",
    "print(\"OUTPUT WORKDIR:\", WORKDIR_ORIG)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Locate product name\n",
    "# =====================================================\n",
    "product = os.path.basename(os.path.normpath(PATH_L1B_DATA_INPUT))\n",
    "\n",
    "# =====================================================\n",
    "# Locate XML\n",
    "# =====================================================\n",
    "xml_list = glob.glob(\n",
    "    os.path.join(\n",
    "        PATH_L1B_DATA_INPUT,\n",
    "        \"DATASTRIP\",\n",
    "        \"S2*\",\n",
    "        \"S2*_MTD_L1B_DS_*.xml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if len(xml_list) == 0:\n",
    "    raise RuntimeError(\"No DATASTRIP MTD XML found\")\n",
    "\n",
    "xml_path = xml_list[0]\n",
    "xml_name = os.path.basename(xml_path)\n",
    "\n",
    "# XML path inside docker (relative, after cd)\n",
    "xml_docker = f\"./{xml_name}\"\n",
    "\n",
    "print(\"Using XML:\", xml_path)\n",
    "\n",
    "# =====================================================\n",
    "# Locate VRTs\n",
    "# =====================================================\n",
    "vrt_list = glob.glob(\n",
    "    os.path.join(PATH_L1B_DATA_INPUT, \"DATASTRIP\", \"S2*\", \"GEO_DATA\", \"*.vrt\")\n",
    ")\n",
    "\n",
    "if not vrt_list:\n",
    "    raise RuntimeError(\"No VRT files found in GEO_DATA\")\n",
    "\n",
    "vrt_names = [os.path.splitext(os.path.basename(v))[0] for v in vrt_list]\n",
    "\n",
    "print(\"Found VRTs:\", vrt_names)\n",
    "\n",
    "# =====================================================\n",
    "# Output directories\n",
    "# =====================================================\n",
    "OUTDIR = os.path.join(WORKDIR_ORIG, \"DATA\", \"GDAL_OUTPUT_ORTHO\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "OUTDIR_DOCKER = \"/workspace/DATA/GDAL_OUTPUT_ORTHO\"\n",
    "\n",
    "# =====================================================\n",
    "# Build GDAL docker\n",
    "# =====================================================\n",
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "dockerfile_dir = os.path.abspath(os.path.join(\n",
    "    notebook_dir,\n",
    "    \"src\",\n",
    "    \"gdal-latest\"\n",
    "))\n",
    "\n",
    "print(\"\\n=== BUILDING GDAL LATEST CONTAINER ===\\n\")\n",
    "cmd_build = [\n",
    "    \"docker\", \"build\",\n",
    "    \"--platform=linux/amd64\",\n",
    "    \"-t\", \"gdal-latest\",\n",
    "    dockerfile_dir\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_build), \"\\n\")\n",
    "subprocess.run(cmd_build, check=True)\n",
    "print(\"GDAL image built successfully.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# Generate gdal_ortho.sh\n",
    "# =====================================================\n",
    "gdal_script_path = os.path.join(WORKDIR_ORIG, \"src\", \"gdal_ortho.sh\")\n",
    "os.makedirs(os.path.dirname(gdal_script_path), exist_ok=True)\n",
    "\n",
    "vrt_array = \" \".join([f'\"{v}\"' for v in vrt_names])\n",
    "\n",
    "workspace_input = \"/workspace_tmp\" if use_tmp else \"/workspace\"\n",
    "with open(gdal_script_path, \"w\") as f:\n",
    "    f.write(f\"\"\"#!/bin/bash\n",
    "set +e\n",
    "\n",
    "cd {workspace_input}/DATA/{product}\n",
    "\n",
    "OUT_ORTHO=\"/workspace/DATA/GDAL_OUTPUT_ORTHO\"\n",
    "OUT_MOSAIC=\"/workspace/DATA/GDAL_OUTPUT_MOSAIC\"\n",
    "\n",
    "mkdir -p \"$OUT_ORTHO\"\n",
    "mkdir -p \"$OUT_MOSAIC\"\n",
    "\n",
    "XML=\"{xml_docker}\"\n",
    "\n",
    "echo \"=== ORTHORECTIFICATION ===\"\n",
    "\n",
    "for VRT in {vrt_array}; do\n",
    "    BASENAME=\"$VRT\"\n",
    "    OUT=\"$OUT_ORTHO/${{BASENAME}}_ortho.tif\"\n",
    "\n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Processing VRT: $VRT\"\n",
    "    echo \"----------------------------------------\"\n",
    "\n",
    "    rm -f \"$OUT\"\n",
    "\n",
    "    gdalwarp \\\\\n",
    "        SENTINEL2_L1B_WITH_GEOLOC:./$XML:$VRT \\\\\n",
    "        \"$OUT\" \\\\\n",
    "        -t_srs EPSG:{UTM_EPSG} \\\\\n",
    "        -te {LOCATION[\"ul_x\"]} {LOCATION[\"lr_y\"]} {LOCATION[\"lr_x\"]} {LOCATION[\"ul_y\"]} \\\\\n",
    "        -r bilinear \\\\\n",
    "        -co COMPRESS=LZW \\\\\n",
    "        -co TILED=YES \\\\\n",
    "        -overwrite\n",
    "\n",
    "    echo \"\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== MOSAIC GENERATION ===\"\n",
    "echo \"\"\n",
    "\n",
    "for BAND in {\" \".join(ORTHO_SETTINGS[\"keep_bands\"])}; do\n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Creating mosaic for band: $BAND\"\n",
    "    echo \"----------------------------------------\"\n",
    "\n",
    "    INPUT_FILES=($(ls $OUT_ORTHO/*_${{BAND}}_ortho.tif 2>/dev/null))\n",
    "\n",
    "    if [ ${{#INPUT_FILES[@]}} -eq 0 ]; then\n",
    "        echo \"No ortho images found for band $BAND\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    OUTPUT=\"$OUT_MOSAIC/ORTHO_mosaic_${{BAND}}.tif\"\n",
    "\n",
    "    gdalwarp \\\\\n",
    "        \"${{INPUT_FILES[@]}}\" \\\\\n",
    "        \"$OUTPUT\" \\\\\n",
    "        -r bilinear \\\\\n",
    "        -dstnodata 0 \\\\\n",
    "        -srcnodata 0 \\\\\n",
    "        -multi \\\\\n",
    "        -wm 2048 \\\\\n",
    "        -overwrite \\\\\n",
    "        -co COMPRESS=LZW \\\\\n",
    "        -co TILED=YES \\\\\n",
    "        -ot UInt16\n",
    "\n",
    "    echo \" Mosaic written -> $OUTPUT\"\n",
    "    echo \"\"\n",
    "done\n",
    "\n",
    "echo \"=== GDAL processing complete ===\"\n",
    "\"\"\")\n",
    "\n",
    "os.chmod(gdal_script_path, 0o755)\n",
    "print(\"Generated:\", gdal_script_path)\n",
    "\n",
    "# =====================================================\n",
    "# Run GDAL docker\n",
    "# =====================================================\n",
    "print(\"\\n=== RUNNING GDAL PROCESSING ===\\n\")\n",
    "\n",
    "cmd_run = [\n",
    "    \"docker\", \"run\",\n",
    "    \"--rm\",\n",
    "    \"-v\", f\"{WORKDIR_ORIG}:/workspace\",\n",
    "]\n",
    "\n",
    "if use_tmp:\n",
    "    cmd_run += [\"-v\", f\"{WORKDIR_INPUT}:/workspace_tmp\"]\n",
    "\n",
    "cmd_run += [\n",
    "    \"gdal-latest\",\n",
    "    \"/workspace/src/gdal_ortho.sh\"\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_run), \"\\n\")\n",
    "subprocess.run(cmd_run, check=True)\n",
    "print(\"\\nGDAL ortho + mosaic complete.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# Cleanup docker image\n",
    "# =====================================================\n",
    "print(\"Removing gdal-latest image...\\n\")\n",
    "subprocess.run([\"docker\", \"rmi\", \"-f\", \"gdal-latest\"], check=True)\n",
    "print(\"GDAL image removed.\\n\")\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# Cleanup TMP files\n",
    "# =====================================================\n",
    "\n",
    "if DELETE_TMP_FILES and use_tmp:\n",
    "    print(\"Deleting WORKDIR_TMP:\", WORKDIR_INPUT)\n",
    "    if os.path.exists(WORKDIR_INPUT):\n",
    "        shutil.rmtree(WORKDIR_INPUT)\n",
    "    print(\"WORKDIR_TMP deleted.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
