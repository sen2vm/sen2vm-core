{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8bbfd73",
   "metadata": {},
   "source": [
    "# === USER CONFIGURATION ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # === USER CONFIGURATION ===\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, please read the README_Notebooks.md file before\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "WORKDIR = \"PATH/TO/WORKDIR\"  # Working directory for sen2vm processing\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, the structure of the WORKDIR shall respect the one described in README_Notebooks.md\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "# Path to downloaded product\n",
    "PATH_L1B_DATA = \"PATH/TO/L1B/PRODUCT\"\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, for now PATH_L1B_DATA shall be inside WORKID/DATA and respect the one described in README_Notebooks.md\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "# Output folder chosen by the user\n",
    "INVERSE_OUTPUT_FOLDER = \"PATH/TO/OUTPUT/FOLDER\"  # Used only if GRID_MODE = \"inverse\"\n",
    "\n",
    "\n",
    "# === SEN2VM OPTIONS ===\n",
    "\n",
    "GRID_MODE = \"direct\"  # direct or inverse\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "# /!\\/!\\/!\\ CAREFUL, for now only direct shall be used with a full product (no missing granules)\n",
    "# /!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\/!\\\n",
    "\n",
    "UTM_EPSG = 32636 # UTM zone EPSG code for the ROI\n",
    "LOCATION = {\n",
    "    \"ul_x\": 210000,   \n",
    "    \"ul_y\": 3504541,  \n",
    "    \"lr_x\": 436397,  \n",
    "    \"lr_y\": 3296817 \n",
    "}\n",
    "\n",
    "# Grid step in pixels for sen2vm grid generation \n",
    "# These values represent the grid step in pixels, typically DEM resolution / 2\n",
    "# For S2 with DEM90, this gives approximately: 45m/10m=4.5, 45m/20m=2.25, 45m/60m≈1\n",
    "STEPS = {\n",
    "    \"10m_bands\": 4.5,   # Grid step: ~DEM90/2 = 45m/10m = 4.5 pixels (one point every 4.5 pixels at 10m)\n",
    "    \"20m_bands\": 2.25,  # Grid step: ~DEM90/2 = 45m/20m = 2.25 pixels (one point every 2.25 pixels at 20m)\n",
    "    \"60m_bands\": 1      # Grid step: ~DEM90/2 = 45m/60m ≈ 1 pixel (one point every 1 pixel at 60m, or 2 is also fine)\n",
    "}\n",
    "\n",
    "\n",
    "# === GDAL ORTHO OPTIONS ===\n",
    "\n",
    "ORTHO_SETTINGS = {\n",
    "    \"keep_bands\": [\"B03\"],  # list of bands to keep for orthorectification\n",
    "    \"keep_detectors\": [\"11\",\"10\"]  # list of the detectors to keep \n",
    "}\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e95184",
   "metadata": {},
   "source": [
    "# GIPP database download (Optionnal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e410c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GIPPs ===\n",
    "# This step is optionnal if the Database was already downloaded or if you want to use your own GIPP\n",
    "# Please note that this current notebook will search for a subfolder with mission S2[A/B/C] inside the GIPP folder\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import tarfile\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "gipp_dir = os.path.join(WORKDIR, \"DATA\")     \n",
    "os.makedirs(gipp_dir, exist_ok=True)             \n",
    "\n",
    "# =====================================================================\n",
    "# 1) CLONE sen2vm-gipp-database\n",
    "# =====================================================================\n",
    "\n",
    "gipp_repo_name = \"sen2vm-gipp-database\"\n",
    "gipp_repo_path = os.path.join(gipp_dir, gipp_repo_name)\n",
    "\n",
    "# Delete repository if exists\n",
    "if os.path.exists(gipp_repo_path):\n",
    "    print(f\"Removing existing repository: {gipp_repo_path}\")\n",
    "    shutil.rmtree(gipp_repo_path)\n",
    "\n",
    "# Clone repository fresh\n",
    "print(\"Cloning sen2vm-gipp-database...\")\n",
    "!git clone https://github.com/sen2vm/sen2vm-gipp-database.git {gipp_repo_path}\n",
    "print(\"Clone complete.\\n\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"GIPP processing finished successfully.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f616a3",
   "metadata": {},
   "source": [
    "# IERS Download (optionnal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec4d6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === DOWNLOAD IERS ===\n",
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "DATA_DIR = os.path.join(WORKDIR, \"DATA\")\n",
    "\n",
    "if not os.path.exists(DATA_DIR):\n",
    "    raise RuntimeError(f\"DATA directory not found: {DATA_DIR}\")\n",
    "\n",
    "print(\"Bulletin output directory:\", DATA_DIR)\n",
    "\n",
    "# =====================================================================\n",
    "# 0. REMOVE EXISTING IERS BULLETINS\n",
    "# =====================================================================\n",
    "\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.startswith(\"bulletina-\") and f.endswith(\".txt\"):\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        print(\"Removed old bulletin A:\", f)\n",
    "\n",
    "    if f.startswith(\"bulletinb-\") and f.endswith(\".txt\"):\n",
    "        os.remove(os.path.join(DATA_DIR, f))\n",
    "        print(\"Removed old bulletin B:\", f)\n",
    "\n",
    "print(\"Cleanup of old bulletins complete.\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# 2. EXTRACT PRODUCT DATE (FROM DATASTRIP)\n",
    "# =====================================================================\n",
    "\n",
    "datastrip_dir = os.path.join(PATH_L1B_DATA, \"DATASTRIP\")\n",
    "\n",
    "if not os.path.isdir(datastrip_dir):\n",
    "    raise RuntimeError(f\"DATASTRIP directory not found: {datastrip_dir}\")\n",
    "\n",
    "datastrip_entries = os.listdir(datastrip_dir)\n",
    "if not datastrip_entries:\n",
    "    raise RuntimeError(f\"No DATASTRIP found in: {datastrip_dir}\")\n",
    "\n",
    "# Take the first DATASTRIP product\n",
    "datastrip_name = datastrip_entries[0]\n",
    "\n",
    "match = re.search(r\"_S(\\d{8})T\\d{6}_\", datastrip_name)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Could not extract product date from DATASTRIP name.\")\n",
    "\n",
    "product_date_str = match.group(1)\n",
    "\n",
    "year = int(product_date_str[:4])\n",
    "month = int(product_date_str[4:6])\n",
    "day = int(product_date_str[6:8])\n",
    "\n",
    "product_date = datetime(year, month, day)\n",
    "\n",
    "print(\"Product date extracted from DATASTRIP:\", product_date.date(), \"\\n\")\n",
    "\n",
    "# =====================================================================\n",
    "# Roman conversion \n",
    "# =====================================================================\n",
    "\n",
    "def int_to_roman(n):\n",
    "    vals = [\n",
    "        (1000, 'm'), (900, 'cm'), (500, 'd'), (400, 'cd'),\n",
    "        (100, 'c'), (90, 'xc'), (50, 'l'), (40, 'xl'),\n",
    "        (10, 'x'), (9, 'ix'), (5, 'v'), (4, 'iv'), (1, 'i')\n",
    "    ]\n",
    "    res = \"\"\n",
    "    for v, s in vals:\n",
    "        while n >= v:\n",
    "            res += s\n",
    "            n -= v\n",
    "    return res\n",
    "\n",
    "\n",
    "# =====================================================================\n",
    "# Bulletin A\n",
    "# =====================================================================\n",
    "\n",
    "\n",
    "roman_year = int_to_roman(year - 1987)\n",
    "doy = product_date.timetuple().tm_yday\n",
    "index = (doy - 1) // 7 + 1\n",
    "\n",
    "print(f\"Bulletin A Roman year: {roman_year}\")\n",
    "print(f\"Initial weekly index: {index}\\n\")\n",
    "\n",
    "found = False\n",
    "\n",
    "while index > 0:\n",
    "    index_str = f\"{index:03d}\"\n",
    "    url = f\"https://datacenter.iers.org/data/6/bulletina-{roman_year}-{index_str}.txt\"\n",
    "    print(\"Trying bulletin:\", url)\n",
    "\n",
    "    response = requests.get(url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        print(\"Bulletin found:\", index_str)\n",
    "        dest_file = os.path.join(DATA_DIR, f\"bulletina-{roman_year}-{index_str}.txt\")\n",
    "        found = True\n",
    "        break\n",
    "\n",
    "    index -= 1\n",
    "\n",
    "if not found:\n",
    "    raise RuntimeError(\"No Bulletin A available for current or previous weeks.\")\n",
    "\n",
    "with open(dest_file, \"wb\") as f:\n",
    "    f.write(response.content)\n",
    "\n",
    "print(\"Downloaded:\", dest_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e85608d",
   "metadata": {},
   "source": [
    "# Sen2VM configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276592b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE CONFIG.JSON  ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import shutil\n",
    "from numpy import double\n",
    "\n",
    "\n",
    "USERCONF_DIR = os.path.join(WORKDIR, \"UserConf\")\n",
    "os.makedirs(USERCONF_DIR, exist_ok=True)\n",
    "\n",
    "print(\"UserConf directory:\", USERCONF_DIR)\n",
    "\n",
    "GEOID_DIR = os.path.join(WORKDIR, \"DATA\", \"GEOID\")\n",
    "os.makedirs(GEOID_DIR, exist_ok=True)\n",
    "\n",
    "print(\"Geoid directory:\", GEOID_DIR)\n",
    "\n",
    "# =====================================================\n",
    "# 1. Extract mission from DATASTRIP\n",
    "# =====================================================\n",
    "\n",
    "datastrip_dir = os.path.join(PATH_L1B_DATA, \"DATASTRIP\")\n",
    "\n",
    "if not os.path.isdir(datastrip_dir):\n",
    "    raise RuntimeError(f\"DATASTRIP directory not found: {datastrip_dir}\")\n",
    "\n",
    "datastrip_entries = os.listdir(datastrip_dir)\n",
    "if not datastrip_entries:\n",
    "    raise RuntimeError(f\"No DATASTRIP found in: {datastrip_dir}\")\n",
    "\n",
    "datastrip_name = datastrip_entries[0]\n",
    "\n",
    "match = re.match(r\"(S2[A-C])_OPER_\", datastrip_name)\n",
    "if not match:\n",
    "    raise RuntimeError(\"Cannot extract mission (S2A/S2B/S2C) from DATASTRIP name.\")\n",
    "\n",
    "mission = match.group(1)\n",
    "\n",
    "\n",
    "# =====================================================\n",
    "# 2. Docker paths inside /workspace\n",
    "# =====================================================\n",
    "\n",
    "safe_name = os.path.basename(PATH_L1B_DATA)\n",
    "\n",
    "docker_l1b  = f\"/workspace/DATA/{safe_name}\"\n",
    "docker_dem  = \"/workspace/DATA/DEM\"\n",
    "docker_gipp = f\"/workspace/DATA/sen2vm-gipp-database/{mission}\"\n",
    "\n",
    "# =====================================================\n",
    "# 3. Geoid management\n",
    "# =====================================================\n",
    "\n",
    "# Notebook location (NOT relative to CWD)\n",
    "notebook_dir = os.getcwd()\n",
    "\n",
    "# Relative path to DEM_GEOID from notebook\n",
    "internal_geoid_dir = os.path.abspath(os.path.join(\n",
    "    notebook_dir,\n",
    "    \"..\", \"..\", \"src\", \"test\", \"resources\", \"DEM_GEOID\"\n",
    "))\n",
    "\n",
    "# If WORKDIR/DATA/GEOID is empty -> copy internal files\n",
    "if len(os.listdir(GEOID_DIR)) == 0:\n",
    "    print(\"GEOID directory is empty -> copying default geoid files...\")\n",
    "    for f in os.listdir(internal_geoid_dir):\n",
    "        src = os.path.join(internal_geoid_dir, f)\n",
    "        dst = os.path.join(GEOID_DIR, f)\n",
    "        shutil.copy(src, dst)\n",
    "\n",
    "# Detect .gtx inside GEOID_DIR\n",
    "geoid_files = [f for f in os.listdir(GEOID_DIR) if f.lower().endswith(\".gtx\")]\n",
    "if len(geoid_files) == 0:\n",
    "    raise RuntimeError(\"No .gtx geoid file found in WORKDIR/DATA/GEOID.\")\n",
    "\n",
    "docker_geoid = f\"/workspace/DATA/GEOID/{geoid_files[0]}\"\n",
    "\n",
    "# =====================================================\n",
    "# 4. Locate IERS bulletin on host\n",
    "# =====================================================\n",
    "\n",
    "DATA_DIR = os.path.join(WORKDIR, \"DATA\")\n",
    "iers_host = None\n",
    "\n",
    "for f in os.listdir(DATA_DIR):\n",
    "    if f.startswith(\"bulletin\"):\n",
    "        iers_host = os.path.join(DATA_DIR, f)\n",
    "        break\n",
    "\n",
    "if iers_host is None:\n",
    "    raise RuntimeError(\"IERS bulletin not found inside WORKDIR/DATA directory.\")\n",
    "\n",
    "docker_iers = \"/workspace/DATA/\" + os.path.basename(iers_host)\n",
    "\n",
    "# =====================================================\n",
    "# 5. Build config dictionary\n",
    "# =====================================================\n",
    "\n",
    "config = {\n",
    "    \"l1b_product\": docker_l1b,\n",
    "    \"gipp_folder\": docker_gipp,\n",
    "    \"auto_gipp_selection\": True,\n",
    "    \"grids_overwriting\": True,\n",
    "    \"dem\": docker_dem,\n",
    "    \"geoid\": docker_geoid,\n",
    "    \"iers\": docker_iers,\n",
    "    \"operation\": GRID_MODE,\n",
    "    \"deactivate_available_refining\": False,\n",
    "    \"steps\": {\n",
    "        \"10m_bands\": STEPS[\"10m_bands\"],\n",
    "        \"20m_bands\": STEPS[\"20m_bands\"],\n",
    "        \"60m_bands\": STEPS[\"60m_bands\"]\n",
    "    },\n",
    "    \"export_alt\": True\n",
    "}\n",
    "\n",
    "# Add inverse block only if needed\n",
    "if GRID_MODE == \"inverse\":\n",
    "    config[\"inverse_location_additional_info\"] = {\n",
    "        \"ul_x\": double(LOCATION[\"ul_x\"]),\n",
    "        \"ul_y\": double(LOCATION[\"ul_y\"]),\n",
    "        \"lr_x\": double(LOCATION[\"lr_x\"]),\n",
    "        \"lr_y\": double(LOCATION[\"lr_y\"]),\n",
    "        \"referential\": f\"EPSG:{UTM_EPSG}\",\n",
    "        \"output_folder\": \"/workspace/DATA/Output\"\n",
    "    }\n",
    "\n",
    "# =====================================================\n",
    "# Save config.json\n",
    "# =====================================================\n",
    "\n",
    "config_path = os.path.join(USERCONF_DIR, \"config.json\")\n",
    "\n",
    "with open(config_path, \"w\") as f:\n",
    "    json.dump(config, f, indent=4)\n",
    "\n",
    "print(\"Configuration file generated:\")\n",
    "print(config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b1da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === GENERATE PARAMS.JSON ===\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "USERCONF_DIR = os.path.join(WORKDIR, \"UserConf\")\n",
    "os.makedirs(USERCONF_DIR, exist_ok=True)\n",
    "\n",
    "print(\"UserConf directory:\", USERCONF_DIR)\n",
    "\n",
    "# =====================================================\n",
    "# Locate GRANULE folders\n",
    "# =====================================================\n",
    "GR_TARGET_DIR = os.path.join(PATH_L1B_DATA, \"GRANULE\")\n",
    "\n",
    "if not os.path.exists(GR_TARGET_DIR):\n",
    "    raise RuntimeError(\"GRANULE directory not found inside L1B SAFE.\")\n",
    "\n",
    "granule_folders = [\n",
    "    os.path.join(GR_TARGET_DIR, d)\n",
    "    for d in os.listdir(GR_TARGET_DIR)\n",
    "    if os.path.isdir(os.path.join(GR_TARGET_DIR, d))\n",
    "]\n",
    "\n",
    "print(\"Found\", len(granule_folders), \"granule folders.\")\n",
    "\n",
    "# =====================================================\n",
    "# Extract detectors and bands from JP2\n",
    "# =====================================================\n",
    "detectors = set()\n",
    "bands = set()\n",
    "\n",
    "pattern = r\"_D(\\d+)_B(\\d{1,2}[A]?)\\.jp2$\"\n",
    "\n",
    "for granule in granule_folders:\n",
    "    img_data_dir = os.path.join(granule, \"IMG_DATA\")\n",
    "\n",
    "    if not os.path.isdir(img_data_dir):\n",
    "        continue\n",
    "\n",
    "    for fname in os.listdir(img_data_dir):\n",
    "        match = re.search(pattern, fname)\n",
    "        if match:\n",
    "            detectors.add(match.group(1))\n",
    "            bands.add(f\"B{match.group(2)}\")\n",
    "\n",
    "detectors = sorted(detectors)\n",
    "bands = sorted(bands)\n",
    "\n",
    "print(\"Detected detectors:\", detectors)\n",
    "print(\"Detected bands:\", bands)\n",
    "\n",
    "# =====================================================\n",
    "# Write params.json\n",
    "# =====================================================\n",
    "\n",
    "# Validate keep_detectors: must be a non-empty list (user requirement)\n",
    "if not isinstance(ORTHO_SETTINGS.get(\"keep_detectors\"), list) or len(ORTHO_SETTINGS[\"keep_detectors\"]) == 0:\n",
    "    raise RuntimeError(\"ORTHO_SETTINGS['keep_detectors'] must be a non-empty list of detector ids (e.g. ['01','02']).\")\n",
    "\n",
    "# Filter detectors to only include those selected by the user\n",
    "keep_detectors_list = ORTHO_SETTINGS[\"keep_detectors\"]\n",
    "selected_detectors = [d for d in keep_detectors_list if d in detectors]\n",
    "\n",
    "# Validate that all requested detectors exist\n",
    "missing_detectors = [d for d in keep_detectors_list if d not in detectors]\n",
    "if missing_detectors:\n",
    "    raise RuntimeError(f\"Some requested detectors are not available in the product: {missing_detectors}. Available detectors: {detectors}\")\n",
    "\n",
    "# Only include selected detectors and bands in params.json\n",
    "params = {\n",
    "    \"detectors\": sorted(selected_detectors),\n",
    "    \"bands\": sorted(ORTHO_SETTINGS[\"keep_bands\"])\n",
    "}\n",
    "\n",
    "params_path = os.path.join(USERCONF_DIR, \"params.json\")\n",
    "\n",
    "with open(params_path, \"w\") as f:\n",
    "    json.dump(params, f, indent=4)\n",
    "\n",
    "print(\"params.json written to:\", params_path)\n",
    "print(f\"  - Detectors: {params['detectors']}\")\n",
    "print(f\"  - Bands: {params['bands']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f67fe",
   "metadata": {},
   "source": [
    "# Sen2VM run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572bd72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === RUN SEN2VM (Docker: BUILD + RUN + CLEAN) ===\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "\n",
    "dockerfile_dir = os.path.abspath(os.path.join(\n",
    "    notebook_dir,\n",
    "    \"..\", \"..\"\n",
    "))\n",
    "\n",
    "config_inside = \"/workspace/UserConf/config.json\"\n",
    "params_inside = \"/workspace/UserConf/params.json\"\n",
    "\n",
    "# =====================================================\n",
    "# 1. BUILD DOCKER IMAGE\n",
    "# =====================================================\n",
    "\n",
    "print(f\"Building Docker image 'sen2vm' from: {dockerfile_dir}\")\n",
    "\n",
    "cmd_build = [\n",
    "    \"docker\", \"build\",\n",
    "    \"-t\", \"sen2vm\",\n",
    "    dockerfile_dir\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_build), \"\\n\")\n",
    "subprocess.run(cmd_build, check=True)\n",
    "print(\"Docker image built successfully.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. RUN SEN2VM CONTAINER\n",
    "# =====================================================\n",
    "\n",
    "cmd_run = [\n",
    "    \"docker\", \"run\",\n",
    "    \"--rm\",\n",
    "    \"-v\", f\"{WORKDIR}:/workspace\",  \n",
    "    \"sen2vm\",\n",
    "    \"-c\", config_inside,\n",
    "    \"-p\", params_inside\n",
    "]\n",
    "\n",
    "print(\"Running Docker container...\\n\")\n",
    "print(\"Command:\", \" \".join(cmd_run), \"\\n\")\n",
    "\n",
    "subprocess.run(cmd_run, check=True)\n",
    "\n",
    "print(\"\\nDocker execution complete.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. REMOVE DOCKER IMAGE\n",
    "# =====================================================\n",
    "\n",
    "print(\"Removing Docker image 'sen2vm'...\")\n",
    "\n",
    "subprocess.run([\"docker\", \"rmi\", \"-f\", \"sen2vm\"], check=True)\n",
    "\n",
    "print(\"Docker image removed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54044d2",
   "metadata": {},
   "source": [
    "# Generate Orthorectification images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a091ccaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import glob\n",
    "import json\n",
    "import re\n",
    "\n",
    "# =====================================================\n",
    "# Locate product name\n",
    "# =====================================================\n",
    "product = os.path.basename(os.path.normpath(PATH_L1B_DATA))\n",
    "\n",
    "# =====================================================\n",
    "# Locate XML\n",
    "# =====================================================\n",
    "xml_list = glob.glob(\n",
    "    os.path.join(\n",
    "        PATH_L1B_DATA,\n",
    "        \"DATASTRIP\",\n",
    "        \"S2*\",\n",
    "        \"S2*_MTD_L1B_DS_*.xml\"\n",
    "    )\n",
    ")\n",
    "\n",
    "if len(xml_list) == 0:\n",
    "    raise RuntimeError(\"No DATASTRIP MTD XML found\")\n",
    "\n",
    "xml_path = xml_list[0]\n",
    "xml_name = os.path.basename(xml_path)\n",
    "\n",
    "# XML path inside docker (relative, after cd)\n",
    "xml_docker = f\"./{xml_name}\"\n",
    "\n",
    "print(\"Using XML:\", xml_path)\n",
    "\n",
    "# =====================================================\n",
    "# Read params.json to get selected bands and detectors\n",
    "# =====================================================\n",
    "params_path = os.path.join(WORKDIR, \"UserConf\", \"params.json\")\n",
    "if not os.path.exists(params_path):\n",
    "    raise RuntimeError(f\"params.json not found at {params_path}\")\n",
    "\n",
    "with open(params_path, \"r\") as f:\n",
    "    params = json.load(f)\n",
    "\n",
    "selected_bands = params.get(\"bands\", [])\n",
    "selected_detectors = params.get(\"detectors\", [])\n",
    "\n",
    "print(f\"Bands from params.json: {selected_bands}\")\n",
    "print(f\"Detectors from params.json: {selected_detectors}\")\n",
    "\n",
    "# =====================================================\n",
    "# Locate VRTs\n",
    "# =====================================================\n",
    "vrt_list = glob.glob(\n",
    "    os.path.join(PATH_L1B_DATA, \"DATASTRIP\", \"S2*\", \"GEO_DATA\", \"*.vrt\")\n",
    ")\n",
    "\n",
    "if not vrt_list:\n",
    "    raise RuntimeError(\"No VRT files found in GEO_DATA\")\n",
    "\n",
    "print(f\"Total VRT files found: {len(vrt_list)}\")\n",
    "\n",
    "# =====================================================\n",
    "# Filter VRTs based on params.json\n",
    "# =====================================================\n",
    "vrt_names = []\n",
    "vrt_pattern = r\"_D(\\d+)_B(\\d{1,2}[A]?)$\"\n",
    "\n",
    "for vrt_path in vrt_list:\n",
    "    vrt_name = os.path.splitext(os.path.basename(vrt_path))[0]\n",
    "    match = re.search(vrt_pattern, vrt_name)\n",
    "    \n",
    "    if match:\n",
    "        detector = match.group(1)\n",
    "        band = f\"B{match.group(2)}\"\n",
    "        \n",
    "        # Check if this VRT matches selected bands and detectors\n",
    "        if detector in selected_detectors and band in selected_bands:\n",
    "            vrt_names.append(vrt_name)\n",
    "            print(f\"  ✓ Keep: {vrt_name} (detector={detector}, band={band})\")\n",
    "    else:\n",
    "        print(f\"  Warning: Could not parse detector/band from {vrt_name}\")\n",
    "\n",
    "print(f\"\\nFiltered VRTs to process: {len(vrt_names)}\")\n",
    "print(f\"VRT list: {vrt_names}\")\n",
    "\n",
    "# =====================================================\n",
    "# Output directories\n",
    "# =====================================================\n",
    "OUTDIR = os.path.join(WORKDIR, \"DATA\", \"GDAL_OUTPUT_ORTHO\")\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "OUTDIR_DOCKER = \"/workspace/DATA/GDAL_OUTPUT_ORTHO\"\n",
    "\n",
    "# =====================================================\n",
    "# Build GDAL docker\n",
    "# =====================================================\n",
    "notebook_dir = os.path.dirname(os.getcwd())\n",
    "dockerfile_dir = os.path.abspath(os.path.join(\n",
    "    notebook_dir,\n",
    "    \"src\",\n",
    "    \"gdal-latest\"\n",
    "))\n",
    "\n",
    "print(\"\\n=== BUILDING GDAL LATEST CONTAINER ===\\n\")\n",
    "cmd_build = [\n",
    "    \"docker\", \"build\",\n",
    "    \"--platform=linux/amd64\",\n",
    "    \"-t\", \"gdal-latest\",\n",
    "    dockerfile_dir\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_build), \"\\n\")\n",
    "subprocess.run(cmd_build, check=True)\n",
    "print(\"GDAL image built successfully.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# Generate gdal_ortho.sh\n",
    "# =====================================================\n",
    "gdal_script_path = os.path.join(WORKDIR, \"src\", \"gdal_ortho.sh\")\n",
    "os.makedirs(os.path.dirname(gdal_script_path), exist_ok=True)\n",
    "\n",
    "# Verify we have VRTs to process\n",
    "if len(vrt_names) == 0:\n",
    "    raise RuntimeError(f\"No VRT files match the selected bands {selected_bands} and detectors {selected_detectors}\")\n",
    "\n",
    "print(f\"\\n=== GENERATING GDAL SCRIPT ===\")\n",
    "print(f\"VRTs to process: {len(vrt_names)}\")\n",
    "for v in vrt_names:\n",
    "    print(f\"  - {v}\")\n",
    "\n",
    "vrt_array = \" \".join([f'\"{v}\"' for v in vrt_names])\n",
    "\n",
    "# Band resolution in meters for resampling\n",
    "# Note: This is different from STEPS which is the grid step in pixels\n",
    "# STEPS is used for grid generation, while these values are for resampling resolution\n",
    "resolution_10m = 10  # 10 meters for 10m bands\n",
    "resolution_20m = 20  # 20 meters for 20m bands\n",
    "resolution_60m = 60  # 60 meters for 60m bands\n",
    "\n",
    "with open(gdal_script_path, \"w\") as f:\n",
    "    f.write(f\"\"\"#!/bin/bash\n",
    "set +e\n",
    "\n",
    "cd /workspace/DATA/{product}\n",
    "\n",
    "OUT_ORTHO=\"/workspace/DATA/GDAL_OUTPUT_ORTHO\"\n",
    "OUT_MOSAIC=\"/workspace/DATA/GDAL_OUTPUT_MOSAIC\"\n",
    "\n",
    "mkdir -p \"$OUT_ORTHO\"\n",
    "mkdir -p \"$OUT_MOSAIC\"\n",
    "\n",
    "XML=\"{xml_docker}\"\n",
    "\n",
    "# =====================================================\n",
    "# Function to get band resolution in meters \n",
    "# Note: This returns the resampling resolution in meters, not the grid step in pixels\n",
    "# =====================================================\n",
    "get_band_resolution() {{\n",
    "    local vrt_name=\"$1\"\n",
    "    # Extract band from VRT name (e.g., ..._D09_B01 -> B01)\n",
    "    local band=$(echo \"$vrt_name\" | grep -oE '_B[0-9]{{1,2}}[A]?' | sed 's/_//')\n",
    "    \n",
    "    case \"$band\" in\n",
    "        B02|B03|B04|B08)\n",
    "            echo {resolution_10m}  # 10m bands -> 10 meters resolution\n",
    "            ;;\n",
    "        B05|B06|B07|B8A|B11|B12)\n",
    "            echo {resolution_20m}  # 20m bands -> 20 meters resolution\n",
    "            ;;\n",
    "        B01|B09|B10)\n",
    "            echo {resolution_60m}  # 60m bands -> 60 meters resolution\n",
    "            ;;\n",
    "        *)\n",
    "            echo {resolution_10m}  # Default to 10 meters\n",
    "            ;;\n",
    "    esac\n",
    "}}\n",
    "\n",
    "echo \"=== ORTHORECTIFICATION ===\"\n",
    "\n",
    "for VRT in {vrt_array}; do\n",
    "    BASENAME=\"$VRT\"\n",
    "    OUT=\"$OUT_ORTHO/${{BASENAME}}_ortho.tif\"\n",
    "\n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Processing VRT: $VRT\"\n",
    "    echo \"----------------------------------------\"\n",
    "\n",
    "    rm -f \"$OUT\"\n",
    "\n",
    "    # Get resolution for this band (Issue #61)\n",
    "    RESOLUTION=$(get_band_resolution \"$VRT\")\n",
    "    echo \"Band resolution: $RESOLUTION m\"\n",
    "\n",
    "    # Orthorectification with explicit resolution and nodata\n",
    "    gdalwarp \\\\\n",
    "        SENTINEL2_L1B_WITH_GEOLOC:./$XML:$VRT \\\\\n",
    "        \"$OUT\" \\\\\n",
    "        -t_srs EPSG:{UTM_EPSG} \\\\\n",
    "        -te {LOCATION[\"ul_x\"]} {LOCATION[\"lr_y\"]} {LOCATION[\"lr_x\"]} {LOCATION[\"ul_y\"]} \\\\\n",
    "        -r cubic \\\\\n",
    "        -tr $RESOLUTION -$RESOLUTION \\\\\n",
    "        -dstnodata 0 \\\\\n",
    "        -co COMPRESS=LZW \\\\\n",
    "        -co TILED=YES \\\\\n",
    "        -overwrite\n",
    "\n",
    "    echo \"\"\n",
    "done\n",
    "\n",
    "echo \"\"\n",
    "echo \"=== MOSAIC GENERATION ===\"\n",
    "echo \"\"\n",
    "\n",
    "for BAND in {\" \".join(selected_bands)}; do\n",
    "    echo \"----------------------------------------\"\n",
    "    echo \"Creating mosaic for band: $BAND\"\n",
    "    echo \"----------------------------------------\"\n",
    "\n",
    "    INPUT_FILES=($(ls $OUT_ORTHO/*_${{BAND}}_ortho.tif 2>/dev/null))\n",
    "\n",
    "    if [ ${{#INPUT_FILES[@]}} -eq 0 ]; then\n",
    "        echo \"No ortho images found for band $BAND\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    OUTPUT=\"$OUT_MOSAIC/ORTHO_mosaic_${{BAND}}.tif\"\n",
    "\n",
    "    # Use gdal_merge.py instead of gdalwarp to avoid double resampling \n",
    "    # All ortho images should have the same resolution and geometry\n",
    "    # No resampling needed, just assembly\n",
    "    gdal_merge.py \\\\\n",
    "        -o \"$OUTPUT\" \\\\\n",
    "        -of GTiff \\\\\n",
    "        -co COMPRESS=LZW \\\\\n",
    "        -co TILED=YES \\\\\n",
    "        -ot UInt16 \\\\\n",
    "        -n 0 \\\\\n",
    "        -a_nodata 0 \\\\\n",
    "        \"${{INPUT_FILES[@]}}\"\n",
    "\n",
    "    echo \" Mosaic written -> $OUTPUT\"\n",
    "    echo \"\"\n",
    "done\n",
    "\n",
    "echo \"=== GDAL processing complete ===\"\n",
    "\"\"\")\n",
    "\n",
    "os.chmod(gdal_script_path, 0o755)\n",
    "print(\"Generated:\", gdal_script_path)\n",
    "\n",
    "# =====================================================\n",
    "# Run GDAL docker\n",
    "# =====================================================\n",
    "print(\"\\n=== RUNNING GDAL PROCESSING ===\\n\")\n",
    "\n",
    "cmd_run = [\n",
    "    \"docker\", \"run\",\n",
    "    \"--rm\",\n",
    "    \"-v\", f\"{WORKDIR}:/workspace\",\n",
    "    \"gdal-latest\",\n",
    "    \"/workspace/src/gdal_ortho.sh\"\n",
    "]\n",
    "\n",
    "print(\"Command:\", \" \".join(cmd_run), \"\\n\")\n",
    "subprocess.run(cmd_run, check=True)\n",
    "print(\"\\nGDAL ortho + mosaic complete.\\n\")\n",
    "\n",
    "# =====================================================\n",
    "# Cleanup docker image\n",
    "# =====================================================\n",
    "print(\"Removing gdal-latest image...\\n\")\n",
    "subprocess.run([\"docker\", \"rmi\", \"-f\", \"gdal-latest\"], check=True)\n",
    "print(\"GDAL image removed.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
